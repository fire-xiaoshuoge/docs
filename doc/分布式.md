# 分布式

## 分布式系统

### 进程与线程

进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，进程是系统进行资源分配和调度的一个独立单位。线程是进程的一个实体，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源（如程序计数器、一组寄存器和栈），但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。

### 并发

当有多个线程在操作时，如果系统只有一个CPU，则它根本不可能真正同时进行一个以上的线程，它只能把CPU运行时间划分成若干个时间段，再将时间段分配给各个线程执行，在一个时间段的线程代码运行时，其他线程处于挂起状态。这种方式我们称之为并发（Concurrent）。

### 并行

当系统有一个以上的CPU时，则线程的操作有可能非并发。当一个CPU执行一个线程时，另一个CPU可以执行另一个线程，两个线程互不抢占CPU资源，可以同时进行，这种方式我们称之为并行（Parallel）。

### 幂等性

所谓幂等性就是调用1次和调用N次要返回一样的结果。比如一次转账动作，A账户转账1000到B账户，由于网络调用超时，客户端client基于上述保障成功率的原因发起了retry，那么最终应该转账1000还是2000呢，客户的意愿是1000。只需要在设计上加上调用订单号就可以规避这个问题，多次重发，调用的订单号一样，则在服务提供方内部只做一次真实转账动作就行了。

### 分布式系统

分布式系统（*distributed system*）是建立在网络之上的[软件](https://baike.baidu.com/item/软件)系统。正是因为[软件](https://baike.baidu.com/item/软件/12053)的特性，所以分布式系统具有高度的[内聚性](https://baike.baidu.com/item/内聚性/4973441)和透明性。因此，网络和分布式系统之间的区别更多的在于高层[软件](https://baike.baidu.com/item/软件/12053)（特别是[操作系统](https://baike.baidu.com/item/操作系统/192)），而不是硬件。

分布式系统具有以下5个特性：

● 内聚性和透明性：分布式系统是建立在网络之上的软件系统，所以具有高度的内聚性和透明性。

● 可扩展性：分布式系统可以随着业务的增长动态扩展自己的系统组件，从而提高系统整体的处理能力。通常有两种方式：其一，优化系统的性能或者升级硬件，即垂直扩展；其二，增加计算单元（如服务器等）以扩展系统的规模，即水平扩展。

● 可用与可靠性：说直白点，可靠性量化的指标是给定周期内系统无故障运行的平均时间，而可用性量化的指标是给定周期内系统无故障运行的总时间；一个是“平均”时间，一个是“总”时间。

● 高性能：不管是单机系统还是分布式系统，性能始终是关键指标。不同的系统对性能的衡量指标是不同的，最常见的有“高并发”（即单位时间内处理的任务越多越好和“低延迟”（即每个任务的平均处理时间越少越好）。分布式系统的设计初衷便是利用更多的机器，实现更强大的计算和存储能力，即实现高性能。

● 一致性：分布式系统为了提高可用性和可靠性，一般会引入冗余（副本）。为了保证这些节点上的状态一致，分布式系统必须解决一致性问题，其实就是在多个节点集群部署下，如何保证多个节点在给定的时间内，操作或者存储的数据只有一份。

### CAP原则

CAP原则又称CAP定理，指的是在一个分布式系统中，[一致性](https://baike.baidu.com/item/一致性/9840083)（Consistency）、[可用性](https://baike.baidu.com/item/可用性/109628)（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。

AP原则又称CAP定理，指的是在一个[分布式系统](https://baike.baidu.com/item/分布式系统/4905336)中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。

一致性（C）：在[分布式系统](https://baike.baidu.com/item/分布式系统/4905336)中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）

可用性（A）：在集群中一部分节点故障后，[集群](https://baike.baidu.com/item/集群/5486962)整体是否还能响应[客户端](https://baike.baidu.com/item/客户端/101081)的读写请求。（对数据更新具备高可用性）

分区容忍性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。

CAP原则的精髓就是要么AP，要么CP，要么AC，但是不存在CAP。如果在某个分布式系统中数据无副本， 那么系统必然满足强一致性条件， 因为只有独一数据，不会出现数据不一致的情况，此时C和P两要素具备，但是如果系统发生了网络分区状况或者宕机，必然导致某些数据不可以访问，此时可用性条件就不能被满足，即在此情况下获得了CP系统，但是CAP不可同时满足。

### BASE定理

BASE是Basically Available（基本可用）、Soft state（软状态）、Eventually consistent（最终一致性）这四个单词的缩写。在单体架构往分布式架构迁移时，数据一致性要做到满足ACID特性往往比较困难，于是BASE定理就提出在一些非严格要求强一致的应用场景下面，应用可以根据情况采取适当的措施来达到最终一致性，其宗旨是通过牺牲强一致来获取可用性。

### 脑裂

主备是实现高可用的有效方式，但存在一个脑裂问题。脑裂（split-brain），指在一个高可用（HA）系统中，当联系着的两个节点断开联系时，本来为一个整体的系统，分裂为两个独立节点，这时两个节点开始争抢共享资源，结果会导致系统混乱，数据损坏。

### MVCC

MVCC，全称Multiversion concurrency control，翻译为基于多版本并发控制。人们一般把基于锁（比如行级锁）的并发控制机制称成为悲观机制，而把MVCC机制称为乐观机制。由于MVCC是一种宽松的设计，读写相互不阻塞，可以获得较好的并发性能。

### XA

在分布式系统中，每个数据库只能保证自己的数据可以满足ACID（保证强一致性），但它们可能部署在不同的服务器上，只能通过网络进行通信，因此无法准确地知道其他数据库中的事务执行情况。因此，为了解决多个节点之间的协调问题，就需要引入一个协调者负责控制所有节点的操作结果，要么全部成功，要么全部失败。其中，XA协议是一个分布式事务协议，它有两个角色：事务管理者和资源管理者。我们可以把事务管理者理解为协调者，而把资源管理者理解为参与者。

XA协议通过二阶段提交协议保证强一致性。

顾名思义，二阶段提交协议具有两个阶段：第一阶段准备，第二阶段提交。事务管理者（协调者）主要负责控制所有节点的操作结果，包括准备流程和提交流程。第一阶段，事务管理者（协调者）向资源管理者（参与者）发起准备指令，询问资源管理者（参与者）预提交是否成功。如果资源管理者（参与者）可以完成，则执行操作，并不提交，最后给出自己的响应结果，是预提交成功还是预提交失败。第二阶段，如果全部资源管理者（参与者）都回复预提交成功，则资源管理者（参与者）正式提交命令。如果其中有一个资源管理者（参与者）回复预提交失败，则事务管理者（协调者）向所有的资源管理者（参与者）发起回滚命令。举个案例，现在我们有一个事务管理者（协调者）、三个资源管理者（参与者），那么这个事务中我们需要保证这三个参与者在事务过程中的数据的强一致性。事务管理者（协调者）发起准备指令预判它们是否已经预提交成功了，如果全部回复预提交成功，那么事务管理者（协调者）正式发起提交命令执行数据的变更。

需要注意的是，虽然二阶段提交协议的方式为保证强一致性提出了一套解决方案，但仍然存在一些问题。其一，事务管理者（协调者）主要负责控制所有节点的操作结果，包括准备流程和提交流程，但整个流程是同步的，所以事务管理者（协调者）必须等待每一个资源管理者（参与者）返回操作结果后才能进行下一步操作。这样就非常容易造成“同步阻塞”问题。其二，单点故障也是需要认真考虑的问题。事务管理者（协调者）和资源管理者（参与者）都可能出现宕机，如果资源管理者（参与者）出现故障则无法响应而一直等待，事务管理者（协调者）出现故障则事务流程就失去了控制者，换句话说，就是整个流程会一直阻塞，甚至极端的情况下，一部分资源管理者（参与者）数据执行提交，另一部分没有执行提交，也会出现数据不一致性。此时，读者会提出疑问：这些问题应该都是小概率事情，一般是不会产生的。是的，但对于分布式事务场景，我们不仅需要考虑正常逻辑流程，还需要关注小概率的异常场景，如果我们对异常场景缺乏处理方案，则可能出现数据的不一致性，那么后期靠人工干预处理会是一个成本非常大的任务，此外，对于交易的核心链路也许就不是数据问题，而是更加严重的资源损耗问题。

二阶段提交协议存在诸多潜在问题，因此三阶段提交协议出台了。三阶段提交协议是二阶段提交协议的改良版本，它与二阶段提交协议的不同之处在于引入了超时机制来解决“同步阻塞”问题，此外加入了预备阶段，尽可能提早发现无法执行的资源管理者（参与者）并终止事务，如果全部资源管理者（参与者）都可以完成，才发起第二阶段的准备和第三阶段的提交。否则，其中任何一个资源管理者（参与者）回复执行，或者超时等待，那么就终止事务。总结一下，三阶段提交协议包括第一阶段预备、第二阶段准备和第二阶段提交。

三阶段提交协议很好地解决了二阶段提交协议带来的同步阴塞问题，是一个非常有参考意义的解决方案。但是，极小概率的场景下可能会出现数据的不一致性。因为三阶段提交协议引入了超时机制，如果出现资源管理者（参与者）超时场景就会默认提交成功，如果其没有成功执行，或者其他资源管理者（参与者）出现回滚，那么就会出现数据的不一致性。

### TCC

二阶段提交协议和三阶段提交协议解决了分布式事务的问题，但在极端情况下仍然可能存在数据的不一致性，此外它对系统的开销比较大，引入事务管理者（协调者）后，比较容易出现单点瓶颈，以及在业务规模不断变大的情况下，系统可伸缩性也会存在问题。需要注意的是，它是同步操作，因此引入事务后，直到全局事务结束才能释放资源，性能可能是一个很大的问题。因此，在高并发场景下很少使用。阿里的技术人员提出了另外一种解决方案：TCC模式。

TCC模式将一个任务拆分三个操作：Try、Confirm和Cancel。假如，我们有一个Func（）方法，那么在TCC模式中，它就变成了tryFunc（）、confirmFunc（）和cancelFunc（）三个方法。

我们再来梳理TCC模式的流程。第一阶段主业务服务调用全部的从业务服务的Try操作，并且事务管理器记录操作日志。第二阶段，当全部从业务服务都成功时，再执行Confirm操作，否则会执行Cancel逆操作进行回滚。

![image-20200522090332094](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200522090332094.png)

### Paxos

Paxos算法解决的问题正是分布式一致性问题，即一个分布式系统中的各个进程如何就某个值（决议）达成一致。

Paxos算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复。它利用大多数 (Majority) 机制保证了2F+1的容错能力，即2F+1个节点的系统最多允许F个节点同时出现故障。

一个或多个提议进程 (Proposer) 可以发起提案 (Proposal)，Paxos算法使所有提案中的某一个提案，在所有进程中达成一致。系统中的多数派同时认可该提案，即达成了一致。最多只针对一个确定的提案达成一致。

Paxos将系统中的角色分为提议者 (Proposer)，决策者 (Acceptor)，和最终决策学习者 (Learner):

- **Proposer**: 提出提案 (Proposal)。Proposal信息包括提案编号 (Proposal ID) 和提议的值 (Value)。
- **Acceptor**：参与决策，回应Proposers的提案。收到Proposal后可以接受提案，若Proposal获得多数Acceptors的接受，则称该Proposal被批准。
- **Learner**：不参与决策，从Proposers/Acceptors学习最新达成一致的提案（Value）。

![image-20200522093838285](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200522093838285.png)



Paxos算法通过一个决议分为两个阶段（Learn阶段之前决议已经形成）：

1. 第一阶段：Prepare阶段。Proposer向Acceptors发出Prepare请求，Acceptors针对收到的Prepare请求进行Promise承诺。
2. 第二阶段：Accept阶段。Proposer收到多数Acceptors承诺的Promise后，向Acceptors发出Propose请求，Acceptors针对收到的Propose请求进行Accept处理。
3. 第三阶段：Learn阶段。Proposer在收到多数Acceptors的Accept之后，标志着本次Accept成功，决议形成，将形成的决议发送给所有Learners。

Paxos算法流程中的每条消息描述如下：

- **Prepare**: Proposer生成全局唯一且递增的Proposal ID (可使用时间戳加Server ID)，向所有Acceptors发送Prepare请求，这里无需携带提案内容，只携带Proposal ID即可。
- **Promise**: Acceptors收到Prepare请求后，做出“两个承诺，一个应答”。

两个承诺：

\1. 不再接受Proposal ID小于等于（注意：这里是<= ）当前请求的Prepare请求。

\2. 不再接受Proposal ID小于（注意：这里是< ）当前请求的Propose请求。

一个应答：

不违背以前作出的承诺下，回复已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID，没有则返回空值。

- **Propose**: Proposer 收到多数Acceptors的Promise应答后，从应答中选择Proposal ID最大的提案的Value，作为本次要发起的提案。如果所有应答的提案Value均为空值，则可以自己随意决定提案Value。然后携带当前Proposal ID，向所有Acceptors发送Propose请求。
- **Accept**: Acceptor收到Propose请求后，在不违背自己之前作出的承诺下，接受并持久化当前Proposal ID和提案Value。
- **Learn**: Proposer收到多数Acceptors的Accept后，决议形成，将形成的决议发送给所有Learners。

## 分布式 ID 

###  UUID

 UUID（Universally Unique Identifier）：16字节128位，通常以36长度的字符串表示。 

优点：本地生成 ID，不需要进行远程调用，时延低，性能高；

 缺点：UUID 过长，很多场景不适用，比如用 UUID 做数据库索引字段；没有排序，无法保证趋势递增。

### 自增长序列

 数据库自增长序列或字段：最常见的方式，利用数据库，全数据库唯一。

优点：简单，代码方便，性能可以接受；数字 ID 天然排序，对分页或者需要排序的结果很有帮助。

缺点：不同数据库语法和实现不同，数据库迁移的时候或多数据库版本支持的时候需要处理；在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险；在性能达不到要求的情况下，比较难于扩展；如果遇见多个系统需要合并或者涉及到数据迁移会相当痛苦；分表分库的时候会有麻烦。

### Filcker

Filcker 方法：主要思路采用了 MySql 自增长 ID 的机制（auto_increment + replace into）。

优点：充分借助数据库的自增 ID 机制，可靠性高，生成有序的ID；

缺点：ID 生成性能一来单台数据库读写性能；依赖数据库，当数据库异常时整个系统不可用。

### Redis 生成 ID

Redis 生成 ID：主要依赖于 Redis 是单线程的，所以也可以用生成全局唯一的 ID 。可以用 Redis 的原子操作 INCR 和 INCRBY 来实现。

优点：不依赖与数据库，灵活方便，且性能优于数据库；数字 ID 天然排序，对分页或者需要排序的结果很有帮助。

缺点：如果系统中没有 Redis ，还需要引入新的组件，增加系统复杂度；需要编码和配置的工作量比较大。

### 雪花算法

 snowflake 算法：snowflake 是 Twitter 开源的分布式 ID 生成算法，结果是一个 long 型的 ID。其核心思想是：使用 41bit 作为毫秒数，10bit 作为机器的 ID（5个 bit 是数据中心，5个 bit 是机器 ID），12bit 作为毫秒内的流水号 （意味着每个节点在每毫秒可以产生4096个 ID），最后还有个符号位，永远是0。

优点：不依赖与数据库，灵活方便，且性能优于数据库；ID 按照时间在单机上是递增的。

缺点：在单机上是递增的，但是由于涉及到分布式环境，每台机器上的始终不可能完全同步，也许有时候也会出现不适全局递增的情况。

## 分布式锁

### 分布式锁概述

然而，不管是采用Synchronized关键字还是并发操作类Lock等工具的方式，控制并发线程对“共享资源”的访问，它终归只适用于单体应用或者是单一部署的服务实例，而对于分布式部署的系统或者集群部署的服务实例，此种方式将显得力不从心。这是因为这种方式的“锁”很大程度上需要依赖应用系统所在的JDK，像Synchronized、并发操作工具类Lock等都是Java提供给开发者的关键字或者工具。

分布式锁，也是一种锁机制，只不过是专门应对“分布式”的环境而出现的，它并不是一种全新的中间件或者组件，而只是一种机制，一种实现方式，甚至可以说是一种解决方案。它指的是在分布式部署的环境下，通过锁机制让多个客户端或者多个服务进程互斥地对共享资源进行访问，从而避免出现并发安全、数据不一致等问题。

分布式锁的设计与使用，业界普遍有几点要求。

● 排他性：这一点跟单体应用时代加的“锁”是一个道理，即需要保证分布式部署、服务集群部署的环境下，被共享的资源如数据或者代码块在同一时间内只能被一台机器上的一个线程执行。

● 避免死锁：指的是当前线程获取到锁之后，经过一段有限的时间（该时间一般用于执行实际的业务逻辑），一定要被释放（正常情况或者异常情况下释放）。

● 高可用：指的是获取或释放锁的机制必须高可用而且性能极佳。

● 可重入：指的是该分布式锁最好是一把可重入锁，即当前机器的当前线程在彼时如果没有获取到锁，那么在等待一定的时间后一定要保证可以再被获取到。

● 公平锁（可选）：这并非硬性的要求，指的是不同机器的不同线程在获取锁时最好保证几率是一样的，即应当保证来自不同机器的并发线程可以公平获取到锁。

### 实现分布式锁

鉴于这几点要求，目前业界也提供了多种可靠的方式实现分布式锁，其中就包括基于数据库级别的乐观锁、悲观锁、基于Redis的原子操作、基于ZooKeeper的互斥排他锁，以及基于开源框架Redisson的分布式锁，总体上可以概括为图7.8所示。

![image-20200325210746547](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325210746547.png)

● 基于数据库级别的乐观锁：主要是通过在查询、操作共享数据记录时带上一个标识字段version，通过version来控制每次对数据记录执行的更新操作。

● 基于数据库级别的悲观锁：在这里以MySQL的InnoDB引擎为例，它主要是通过在查询共享的数据记录时加上For Update字眼，表示该共享的数据记录已经被当前线程锁住了（行级别锁、表级别锁），只有当该线程操作完成并提交事务之后，才会释放该锁，从而其他线程才能获取到该数据记录。

● 基于Redis的原子操作：主要是通过Redis提供的原子操作SETNX与EXPIRE来实现。SETNX表示只有当Key在Redis不存在时才能设置成功，通常这个Key需要设计为与共享的资源有联系，用于间接地当作“锁”，并采用EXPIRE操作释放获取的锁。

● 基于ZooKeeper的互斥排它锁：这种机制主要是通过ZooKeeper在指定的标识字符串（通常这个标识字符串需要设计为跟共享资源有联系，即可以间接地当作“锁”）下维护一个临时有序的节点列表Node List，并保证同一时刻并发线程访问共享资源时只能有一个最小序号的节点（即代表获取到锁的线程），该节点对应的线程即可执行访问共享资源的操作。

### 乐观锁实现

● 基于数据库级别的乐观锁：主要是通过在查询、操作共享数据记录时带上一个标识字段version，通过version来控制每次对数据记录执行的更新操作。

乐观锁是一种很“佛系”的实现方式，它总是认为不会产生并发的问题，因而每次从数据库中获取数据时总认为不会有其他线程对该数据进行修改，因此不会上锁，但是在更新时其会判断其他线程在这之前有没有对该数据进行修改，通常是采用“版本号version”机制进行实现。“版本号version”机制的执行流程是这样的：当前线程取出数据记录时，会顺带把版本号字段version的值取出来，最后在更新该数据记录时，将该version的取值作为更新的条件。当更新成功之后，同时将版本号version的值加1，而其他同时获取到该数据记录的线程在更新时由于version已经不是当初获取的那个数据记录，因而将更新失败，从而避免并发多线程访问共享数据时出现数据不一致的现象。

![image-20200325211526806](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325211526806.png)

从图7.11中可以得知采用version版本号机制实现“乐观锁”的流程，核心步骤在于“获取数据记录需要把version获取出来，在更新数据记录时需要将version作为匹对条件，并同时将version加1，最终实现version趋势递增的行为”。

### 悲观锁实现

● 基于数据库级别的悲观锁：在这里以MySQL的InnoDB引擎为例，它主要是通过在查询共享的数据记录时加上For Update字眼，表示该共享的数据记录已经被当前线程锁住了（行级别锁、表级别锁），只有当该线程操作完成并提交事务之后，才会释放该锁，从而其他线程才能获取到该数据记录。

悲观锁是一种“消极、悲观”的处理方式，它总是假设事情的发生是在最坏的情况，即每次并发线程在获取数据的时候认为其他线程会对数据进行修改，因而每次在获取数据时都会上锁，而其他线程访问该数据的时候就会发生阻塞的现象，最终只有当前线程释放了该共享资源的锁，其他线程才能获取到锁，并对共享资源进行操作。在传统的关系型数据库中就用到了很多类似悲观锁的机制，比如行锁、表锁、读锁和写锁等，都是在进行操作之前先上锁。除此之外，Java中的Synchronized关键字和ReentrantLock工具类等底层的实现也是参照了“悲观锁”的思想。

![image-20200325211753975](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325211753975.png)

从图7.30中可以看出，当并发请求量比较大的时候，由于产生的每个线程在查询数据的时候都需要上锁，而同一时刻只会有一个线程上锁成功，因此只有当该线程对该共享资源操作完毕并释放锁之后，其他正在等待中的线程才能获取到锁。这种方式将会造成大量的线程发生堵塞的现象，在某种程度上会对DB（数据库）服务器造成一定的压力，从这一角度看，基于数据库级别的悲观锁适用于并发量不大的情况，特别是“读”请求数据量不大的情况。

### Redis实现分布式锁

![image-20200325211844486](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325211844486.png)

● 基于Redis的原子操作：主要是通过Redis提供的原子操作SETNX与EXPIRE来实现。SETNX表示只有当Key在Redis不存在时才能设置成功，通常这个Key需要设计为与共享的资源有联系，用于间接地当作“锁”，并采用EXPIRE操作释放获取的锁。

在Redis的知识体系与底层基础架构中，其实并没有直接提供所谓的“分布式锁”组件，而是间接地借助其原子操作加以实现。之所以其原子操作可以实现分布式锁的功能，主要是得益于Redis的单线程机制，即不管外层应用系统并发了N多个线程，当每个线程都需要使用Redis的某个原子操作时，是需要进行“排队等待”的，即在其底层系统架构中，同一时刻、同一个部署节点中只有一个线程执行某种原子操作。

![image-20200311001108384](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200311001108384.png)

这些缺陷包括以下几点：

● 执行Redis的原子操作EXPIRE时，需要设置Key的过期时间TTL，不同的业务场景设置的过期时间是不同的，但是如果设置不当，将很有可能影响应用系统与Redis服务的性能。

● 采用Redis的原子操作SETNX获取分布式锁时，不具备“可重入”的特性。即当高并发产生多线程时，同一时刻只有一个线程可以获取到锁，从而操作共享资源，而其他的线程将获取锁失败，而且是“永远”失败下去。而有一些业务场景需要要求线程具有“可重入”，则需要在应用程序里添加while(true){}的代码块，即不断地循环等待获取分布式锁，这种方式既不优雅，又很有可能造成应用系统性能“卡顿”的现象。

● 在执行Redis的原子操作SETNX之后EXPIRE操作之前，此时如果Redis的服务节点发生宕机，由于Key没有及时被释放而导致最终很有可能出现“死锁”的现象，即永远不会有其他的线程能够获取到锁（因为Key没有被删除，导致其永久存在）。

### ZooKeeper实现分布式锁

● 基于ZooKeeper的互斥排它锁：这种机制主要是通过ZooKeeper在指定的标识字符串（通常这个标识字符串需要设计为跟共享资源有联系，即可以间接地当作“锁”）下维护一个临时有序的节点列表Node List，并保证同一时刻并发线程访问共享资源时只能有一个最小序号的节点（即代表获取到锁的线程），该节点对应的线程即可执行访问共享资源的操作。

![image-20200311002214708](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200311002214708.png)

ZooKeeper将所有数据存储在内存中，最终构成的数据模型可以看作是一棵树（ZNodeTree），由斜杠“/”进行分割，分割之后的每个分支即为路径，每个路径对应的即为一个ZNode，例如/ZNode1/Node1，每个节点都会保存自己的数据内容及一系列的属性信息。在ZooKeeper中，ZNode可以分为“持久节点”和“临时节点”两种类型。

持久节点，顾名思义，指的是一旦这个ZNode被创建了，除非主动移除这个ZNode，否则它将一直保存在ZooKeeper上。而临时节点则不一样，它的生命周期是和客户端的会话绑定在一起的，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。除此之外，ZooKeeper创建的临时节点ZNode可以带上一个整型的数字，这个特性可以用来创建一系列带有顺序序号标识的临时ZNode。

Watcher监听器：指的是“事件监听器”，由于ZooKeeper允许用户在指定的节点ZNode上注册“监听”事件，因而当该节点触发一些特定的事件时，ZooKeeper服务端即Server会将事件通知到感兴趣的客户端Client上，从而让客户端做出相对应的措施。值得一提的是，这种机制是ZooKeeper实现分布式协调服务的重要特性。

### Redisson实现分布式锁

![image-20200311003629203](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200311003629203.png)



## 分布式缓存

### 缓存

#### 缓存

缓存：存储在计算机上的一个原始数据复制集，以便于访问。

CPU的缓存，是指位于CPU与内存之间的临时存储器，容量比内存小得多但交换速度却比内存要快得多。由于CPU的运算速度要比内存读写速度快很多，CPU总有等待数据的时候，而高速缓存则解决了CPU运算速度与内存读写速度不匹配的矛盾。缓存中的数据是内存中的一部分，且这部分是短时间内CPU即将访问的，当CPU调用数据时，先从缓存中调用，从而加快读取速度。而且，CPU是有多级缓存的，有时候也称为几级流水。

#### 页面缓存

页面缓存有两层含义：一个是页面自身对某些元素或全部元素进行缓存；另一层意思是服务端将静态页面或动态页面的元素进行缓存，然后给客户端使用。这里的页面缓存指的是页面自身的缓存或者离线应用缓存。

#### 浏览器缓存

浏览器缓存是根据一套与服务器约定的规则进行工作的，工作规则很简单：检查以确保副本是最新的，通常只要一次会话。浏览器会在硬盘上专门开辟一个空间来存储资源副本作为缓存。在用户触发“后退”操作或点击一个之前看过的链接的时候，浏览器缓存会很管用。同样，如果访问系统中的同一张图片，该图片可以从浏览器缓存中调出并几乎立即显现出来。

#### APP上的缓存

APP使用数据库缓存的方法：在下载完数据文件后，把文件的相关信息，如URL、路径、下载时间、过期时间等存放到数据库，下次下载的时候根据URL先从数据库中查询，如果查询到当前时间并未过期，就根据路径读取本地文件，从而实现缓存的效果。这种方法具有灵活存放文件的属性，进而提供了很大的扩展性，可以为其他的功能提供良好的支持。需要注意的是，要留心数据库缓存的清理机制。

#### Web代理缓存

Web代理几乎是伴随着互联网诞生的，常用的Web代理分为正向代理、反向代理和透明代理。Web代理缓存是将Web代理作为缓存的一种技术。

#### 边缘缓存

使用Web反向代理服务器和使用正向代理服务器一样，可以拥有缓存的作用，反向代理缓存可以缓存原始资源服务器的资源，而不是每次都要向原始资源服务器请求数据，特别是一些静态的数据，比如图片和文件，很多Web服务器就具备反向代理的功能，比如大名鼎鼎的Nginx。如果这些反向代理服务器能够做到和用户来自同一个网络，那么用户访问反向代理服务器，就会得到很高质量的响应速度，所以可以将这样的反向代理缓存称为边缘缓存。

#### 数据库缓存

数据库属于IO密集型的应用，主要负责数据的管理及存储。数据库缓存是一类特殊的缓存，是数据库自身的缓存机制。大多数数据库不需要配置就可以快速运行，但并没有为特定的需求进行优化。在数据库调优的时候，缓存优化是一项很重要的工作。

#### 缓存算法

在实现缓存应用的时候，需要了解缓存技术中的几个术语。

❑ 缓存命中：当客户发起一个请求时，系统接收到这个请求，如果该请求的数据是在缓存中，这一数据就会被使用，这一行为叫作缓存命中。

❑ 没有命中：cache miss是没有命中。如果缓存中还有存储空间，那么没有命中的对象会被存储到缓存中来。

❑ 存储成本：当没有缓存命中时，系统会从数据库或其他数据源取出数据，然后放入缓存。而把这个数据放入缓存所需要的时间和空间，就是存储成本。

❑ 缓存失效：当存储在缓存中的数据需要更新时，就意味着缓存中的这一数据失效了。

❑ 替代策略：当缓存没有命中时，并且缓存容量已经满了，就需要在缓存中去除一条旧数据，然后加入一条新数据，而到底应该去除哪些数据，就是由替代策略决定的。

替代策略的具体实现就是缓存算法，这里简要介绍一下主流的缓存算法：

（1）Least-Recently-Used（LRU）

换掉最近被请求最少的对象，这种传统策略在实际中应用最广。

（2）Least-Frequently-Used（LFU）

替换掉访问次数最少的缓存，这一策略意图是保留最常用的、最流行的对象，替换掉很少使用的那些数据。

（3）Least Recently Used 2（LRU2）

LRU的变种，把被两次访问过的对象放入缓存池，当缓存池满了之后，会把有两次最少使用的缓存对象去除。

（4）Two Queues（2Q）

Two Queues是LRU的另一个变种，把被访问的数据放到LRU的缓存中，如果这个对象再一次被访问，就把他转移到第二个、更大的LRU缓存，使用了多级缓存的方式。

（5）SIZE

替换占用空间最大的对象，这一策略通过淘汰一个大对象而不是多个小对象来提高命中率。不过，可能有些进入缓存的小对象永远不会再被访问。SIZE策略没有提供淘汰这类对象的机制，也会导致“缓存污染”。

（6）LRU-Threshold

不缓存超过某一size的对象，其他与LRU相同。

（7）Log(Size)+LRU

替换size最大的对象，当size相同时，按LRU进行替换。

（8）Hyper-G

LFU的改进版，同时考虑上次访问时间和对象size。

（9）Pitkow/Recker

替换最近最少使用的对象，除非所有对象都是今天访问过的。如果是这样，则替换掉最大的对象。这一策略试图符合每日访问Web网页的特定模式。这一策略也被建议在每天结束时运行，以释放被“旧的”、最近最少使用的对象占用的空间。

（10）Lowest-Latency-First

替换下载时间最少的文档。显然它的目标是最小化平均延迟。

（11）Hybrid Hybrid

有一个目标是减少平均延迟。对缓存中的每个文档都会计算一个保留效用，保留效用最低的对象会被替换掉。

（12）Lowest Relative Value（LRV）

LRV也是基于计算缓存中文档的保留效用，然后替换保留效用最低的文档。

（13）Adaptive Replacement Cache（ARC）

ARC介于LRU和LFU之间，为了提高效果，由2个LRU组成，第一个包含的条目是最近只被使用过一次的，而第二个LRU包含的是最近被使用过两次的条目，因此，得到了新的对象和常用的对象。ARC能够自我调节，并且是低负载的。

（14）Most Recently Used（MRU）

MRU与LRU是相对，移除最近最多被使用的对象。当一次访问过来的时候，有些事情是无法预测的，并且在缓存系统中找出最少最近使用的对象是一项时间复杂度非常高的运算，这时会考虑MRU，在数据库内存缓存中比较常见。

（15）First in First out（FIFO）

FIFO通过一个队列去跟踪所有的缓存对象，最近最常用的缓存对象放在后面，而更早的缓存对象放在前面，当缓存容量满时，排在前面的缓存对象会被踢走，然后把新的缓存对象加进去。

（16）Random Cache

随机缓存就是随意的替换缓存数据，比FIFO机制好，在某些情况下，甚至比LRU好，但是通常LRU都会比随机缓存更好些。

###  Ehcache

Ehcache是一个用Java实现的使用简单、高速、线程安全的缓存管理类库，其提供了用内存、磁盘文件存储，以及分布式存储等多种灵活的管理方案。

#### 主要特性

Ehcache的主要特点如下：

1）快速，简单。在过去众多的测试中已经表明Ehcache是最快的Java缓存之一，Ehcache的线程机制是为大型高并发系统设计的，而且很多用户都不知道他们正在使用Ehcache，也可以看出使用Ehcache不需要什么复杂的配置，Ehcache的API也易于使用，很容易部署上线和运行。

2）多种缓存策略。提供LRU、LFU和FIFO缓存策略。Ehcache支持基于Cache和基于Element的过期策略，每个Cache的存活时间都是可以设置和控制的。Ehcache提供了LRU、LFU和FIFO缓存淘汰算法，在Ehcache 1.2引入了最少使用和先进先出缓存淘汰算法，构成了完整的缓存淘汰算法。

3）缓存数据有两级。内存和磁盘，因此无须担心容量问题。缓存在内存和硬盘存储可以伸缩到GB, Ehcache为大数据存储做过优化。在大内存的情况下，所有进程可以支持数百GB的吞吐，在单台虚拟机上可以支持多缓存管理器，还可以通过Terracotta服务器矩阵伸缩到数百个节点。

4）缓存数据会在虚拟机重启的过程中写入磁盘。Ehcache是第一个引入缓存数据持久化存储的开源Java缓存框架，缓存的数据可以在机器重启后从磁盘上重新获得，可以根据需要使用cache.flush方法将缓存刷到磁盘上面，极大地方便了Ehcache的使用。

5）可以通过RMI、可插入API等方式进行分布式缓存。分布式缓存的选项包括：

❑ 通过Terracotta的缓存集群：缓存发现是自动完成的，并且有很多选项可以用来调试缓存行为和性能。

❑ 使用RMI、JGroups或者JMS来冗余缓存数据：节点可以通过多播或发现者手动配置。状态更新可以通过RMI连接来异步或者同步完成。

❑ 可靠的分发：使用TCP的内建分发机制。

❑ 缓存API：支持RESTFUL和SOAP二种协议，没有语言限制

6）具有缓存和缓存管理器的侦听接口。

❑ 缓存管理器监听器：允许注册实现了CacheManagerEventListener接口的监听器，方法分别是notifyCacheAdded()和notifyCacheRemoved()。

❑ 缓存事件监听器：允许注册实现了CacheEventListener接口的监听器，它提供了许多对缓存事件发生后的处理机制，notifyElementRemoved/Put/Updated/Expired。

7）提供Hibernate的缓存实现。Hibernate默认二级缓存是不启动的，启动二级缓存通过采用Ehcache来实现。

#### Ehcache架构图

由图4-1可知，Ehcache架构共分为四大部分。

![image-20200325215735062](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325215735062.png)

1）Cache Replication：这个模块主要负责缓存同步的几种实现，主要包括TerraCotta、RMI、JMS和JGroup四种方式。

2）In-Process APIS：这个模块主要包括Ehcache对外常用的API，包括JRuby、Hibernate、JMX、SOAP API、Cache Server五种API。

3）Network APIS：这个模块包括的是Ehcache的通信协议，主要有RESTful API、SOAP API和JMX API等。

4）Ehcache Core，这也是我们最关心的部分，下面列出了关于Core的一些关键技术点：

❑ CacheManager：是缓存管理器，可以通过单例或者多例的方式创建，也是Ehcache的入口类。

❑ Cache：每个CacheManager可以管理多个Cache，每个Cache可以采用hash的方式管理多个Element。

❑ Element：用于存放真正的缓存内容。

❑ SOR(system of record)：可以取到真实数据的组件，可以是真正的业务逻辑、外部接口调用、存放真实数据的数据库等等，缓存就是从SOR中读取或者写入到SOR中去的。

#### 缓存数据过期策略

Ehcache主要提供了三种缓存过期策略：

❑ FIFO：根据数据的写入时间，数据先进先出。

❑ LFU：最少被使用，缓存的元素有一个hit属性，hit值最小的将会被清出缓存。

❑ LRU：最近最少使用，缓存的元素有一个时间戳，当缓存容量满了，而又需要腾出地方来缓存新的元素的时候，那么现有缓存元素中时间戳离当前时间最远的元素将被清出缓存。

#### Ehcache缓存的基本用法

1．创建CacheManager

2．将数据存放到缓存中

3．获取缓存

4．删除缓存

5．修改缓存

6．关闭缓存

#### Spring中使用Ehcache

1．配置文件

2．用Spring注解使用Ehcache缓存

Spring提供了四个方法级的缓存注解分别是：（1）@Cacheable （2）@CachePut （3）@CachEvict  （4）@CacheConfig

#### Ehcache集群

EhCache从1.7版本开始，支持五种集群方案分别是Terracotta、RMI、JMS、JGroup、Ehcache Server，其中RMI、JMS和Ehcache Server是最经常使用的。

1. RMI组播方式

RMI是一种点对点的基于Java对象的通讯方式。EhCache从1.2版本开始就支持RMI方式的缓存集群。在集群环境中EhCache所有缓存对象的键和值都必须是可序列化的，也就是必须实现java.io.Serializable接口，这点在其他集群方式下也是需要遵守的。RMI组播模式如图4-2。

![image-20200325220329577](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325220329577.png)

2. JMS消息方式

JMS是两个应用程序之间进行异步通信的API，它为标准消息协议和消息服务提供了一组通用接口，包括创建、发送、读取消息等，用于支持JAVA应用程序开发，JMS也支持基于事件的通信机制，通过发布事件机制向所有与服务器保持连接的客户端发送消息，在发送消息的时候，接收者不需要在线，等到客户端上线的时候，能保证接收到服务器发送的消息。Ehcache支持JMS消息模式，如图4-3所示：

![image-20200325220348419](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325220348419.png)

JMS的核心就是一个消息队列，每个应用节点都订阅预先定义好的主题，同时，节点有元素更新时，也会发布更新元素到主题中去。各个应用服务器节点通过侦听MQ获取到最新的数据，然后分别更新自己的Ehcache缓存，Ehcache默认支持ActiveMQ，也可以通过自定义组件的方式实现类似Kafka和RabbitMQ等。

3. Cache Server模式

Ehcache也支持缓存服务器集群模式如图4-4所示，缓存数据集中在Ehcache Server中存放，Ehcache Server之间做数据复制。Ehcache Server提供了强大的安全机制和监控功能，Ehcache单实例在内存中可以缓存20GB以上的数据。

![image-20200325220419255](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325220419255.png)

#### Ehcache的适用场景

Ehcache作为本地缓存会有一些场景的使用约束，下面将从三个方面做一下简单介绍帮助大家在选择合适的缓存。

（1）比较少的更新数据表的情况下Ehcache作为Hibernate的缓存时，在进行修改表数据(save, update, delete等等)的时候，EhCache会自动把缓存中关于此表的所有缓存全部删除掉，这样做只是能达到同步，但对于数据经常修改的表来说，可能就失去缓存的意义了。

（2）对并发要求不是很严格的情况多台应用服务器中的缓存是不能进行实时同步的。

（3）对一致性要求不高的情况下因为Ehcache本地缓存的特性，目前无法很好的解决不同服务器间缓存同步的问题，所以在一致性要求高的场合下，建议使用Redis、Memcached等集中式缓存。

###  Guava Cache

Guava Cache和Ehcache一样也是本地缓存，虽然都是本地缓存，但是在细分领域中也还是有不同的应用场景，Guava是Google提供的一套Java工具包，而Guava Cache作为Guava的Cache部分而提供了一套非常完善的本地缓存机制。

Guava Cache适用于以下场景：1．愿意消耗一些本地内存空间来提升速度。2．更新锁定。

Guava Cache是一个全内存的本地缓存实现，它提供了线程安全的实现机制。

Guava Cache有两种创建方式：❑ CacheLoader；❑ Callable callback。

### Memcache

#### Memcache

**memcache**是一套[分布式](https://baike.baidu.com/item/分布式/19276232)的高速缓存系统，由[LiveJournal](https://baike.baidu.com/item/LiveJournal)的Brad Fitzpatrick开发，但目前被许多网站使用以提升网站的访问速度，尤其对于一些大型的、需要频繁访问[数据库](https://baike.baidu.com/item/数据库/103728)的网站访问速度提升效果十分显著 [1] 。这是一套[开放源代码](https://baike.baidu.com/item/开放源代码)[软件](https://baike.baidu.com/item/软件)，以BSD license授权发布。

Memcached是一种基于内存的key-value存储，用来存储小块的任意数据（字符串、对象）。这些数据可以是数据库调用、API调用或者是页面渲染的结果。

Memcached简洁而强大。它的简洁设计便于快速开发，减轻开发难度，解决了大数据量缓存的很多问题。它的API兼容大部分流行的开发语言。

本质上，它是一个简洁的key-value存储系统。

一般的使用目的是，通过缓存数据库查询结果，减少数据库访问次数，以提高动态Web应用的速度、提高可扩展性。

![image-20200325221440806](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325221440806.png)

![image-20200325221451434](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325221451434.png)

#### Memcached特征

Memcached最主要的特征为：协议简单、基于libevent的事件处理、内置内存存储方式、客户端分布式。

❑ 协议简单：Memcached和客户端通信并不使用复杂的XML等格式，而是使用简单的基于文本协议或者二进制协议。

❑ 基于libevent的事件处理：由于epoll, kqueue, /dev/poll每个接口都有自己的特点，程序移植非常困难，libevent这个程序库就应运而生了。他将Linux的epoll、BSD类操作系统的kqueue等事件处理功能封装成统一的接口。Memcached使用libevent库，因此能在Linux、BSD、Solaris等操作系统上发挥其高性能。限于篇幅，这里仅仅对事件处理进行了简要介绍，更多相关内容可以参考Dan Kegel的The C10K Problem。

❑ 内置内存存储方式：为了提高性能，Memcached中保存的数据都存储在Memcached内置的内存存储空间中。由于数据仅存在于内存中，因此重启Memcached或者重启操作系统会导致全部数据消失。另外，内存容量达到指定的值之后，Memcached会自动删除不使用的内存。缓存数据的回收采用的是LRU（Least Recently Used）算法，5.2节将详细介绍内存存储，5.3.1节亦会探讨具体的LRU策略。

❑ Memcached客户端分布式：Memcached尽管是“分布式”缓存服务器，但服务器端并没有分布式功能。各个Memcached实例不会互相通信以共享信息。它的分布式主要是通过客户端实现的，如图5-3所示。

![image-20200325221604181](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200325221604181.png)

service表示某分布式服务，该服务在需要使用Memcached做缓存服务时，会利用Memcached客户端来连接Memcached缓存服务器。Memcached客户端会通过一些路由算法选择具体定向到哪台Memcached缓存服务器。也就是说分布式能力是在客户端代码中实现的。

#### 过期机制

Memcached有两种过期机制，一种是Lazy Expiration，另一种是LRU算法。

❑ Lazy Expiration:Memcached内部不会监视记录是否过期，而是在get时查看记录的时间戳，检查记录是否过期。这种技术被称为lazy（惰性）expiration。因此，Memcached不会在过期监视上耗费CPU时间。

❑ LRU算法：LRU顾名思义就是最近最少使用算法。而LRU又有一系列变种，如表5-4所示。

#### Twemcache

Twemcache（发音“two-em-cache”）是Twitter Memcached的缩写，由Twitter研发团队根据Memcached v.1.4.4开发，并在Twitter内部大量应用的高性能内存对象缓存系统，能够满足大规模实时数据访问的需要。Twemcache在github上的托管地址：https://github.com/twitter/twemcache。

#### Twemproxy

Twemproxy也是Twitter的一个开源项目，是一个单线程代理程序，支持Memcached ASCII协议 和Redis协 议。Twemproxy使用较为广泛，在Twemproxy的Github网 址https://github.com/twitter/Twemproxy给出了在生产环境中使用Twemproxy的用户列表。Twemproxy是用C语言编写的，使用Apache 2.0 License授权。Twemproxy通过引入一个代理层，将应用程序后端的多台Redis或Memcached实例进行统一管理，使应用程序只需要在Twemproxy上进行操作即可，而不用关心后面具体有多少个真实的Redis或Memcached实例。当某个节点宕掉时，Twemproxy可以自动将它从集群中剔除，而当缓存系统恢复服务时，Twemproxy也会自动连接。

#### Mcrouter

Mcrouter是一个基于Memcached协议的路由器，适用于大规模的集群中，在峰值的时候，每秒可以处理接近50亿个请求。

其主要特性如下：

❑ 支持标准的Memcached ASCII协议。

❑ 多个客户端可以通过连接Mcrouter共享后端Memcached的连接池。

❑ 多种hash算法可供选择。

❑ 灵活的路由，支持前缀路由。

❑ 复制模式连接池，写操作复制到连接池中所有实例，读操作从其中一个实例读取。

❑ 支持流量复制，从生产环境复制流量，对新上的集群进行测试。

❑ 在线更新配置。

❑ 支持后端的Memcached健康检测和失败后的自动保护。

❑ 对cold集群热身。

❑ 在后端连接池/集群广播接收到的操作。

❑ 可靠删除。在后端某个实例删除失败时，Mcrouter将操作记录在redolog中，后台线程间隔性地重新执行删除操作。

❑ 多集群支持，拥有丰富的监控和调试命令。

❑ QoS支持。Mcrouter支持根据主机、连接池、集群进行流量控制，可以对任何操作进行流控，例如get/set/delete。还可以对超限流的请求进行拒绝，也可以对流量整形。

❑ 支持超大对象，Mcrouter可以对不能放入Memcached Slab中的超大对象自动切分/重组。

❑ Mcrouter支持本地缓存，实现多级高速缓存。

❑ IPv6和SSL的支持。

###  EVCache

EVCache是一个开源、快速的分布式缓存，是基于Memcached的内存存储和Spymem-cached客户端实现的解决方案，主要用在亚马逊弹性计算云服务（AWS EC2）的基础设施上，为云计算做了优化，能够顺畅而高效地提供数据层服务。

EVCache具有如下的特性：

❑ 分布式的键值对存储，缓存可以跨越多个实例。

❑ 数据可以跨越亚马逊云服务的可用区进行复制。

❑ 通过Netflix内部的命名服务进行注册，自动发现新节点和服务。

❑ 为了存储数据，键是非空字符串，值可以是非空的字节数组，基本类型，或者序列化对象，且小于1 MB。

❑ 作为通用的缓存集群被各种应用使用，支持可选的缓存名称，通过命名空间避免主键冲突。

❑ 一般的缓存命中率在99%以上。

❑ 与Netflix驻留数据框架能够良好协作，典型的访问次序：内存→EVCache→Cassandra/SimpleDB/S3。

### Aerospike

Aerospike是一个分布式的，可扩展的键-值存储的NoSQL数据库。支持灵活的数据模式，并且支持满足ACID特性的事务。其主要的优势是采用混合存储架构，数据索引信息存储在RAM（随机存取存储器）中，而数据本身可以存储在SSD（固态硬盘）或HDD（机械硬盘）上。并且针对采用多核处理器和多处理器机器的现代硬件进行了优化，通过直接硬盘访问（绕过文件系统），可以带来难以置信的性能。主要应用于百G，数T等大规模并且并发在数万以上，对读写性能要求较高的场景，目前主要集中应用在互联网广告行业，如：MediaV, InMobi, eXelate,BuleKai，时趣互动等。

## 分布式存储

### 分布式存储概念

分布式存储系统具有如下几个特性：

可拓展。分布式存储系统可以扩展到几百台甚至几千台的集群规模，而且，随着集群规模的增长，系统整体性能表现为线性增长。

低成本。分布式存储系统的自动容错、自动负载均衡机制使其可以构建在普通PC机之上。另外，线性扩展能力也使得增加、减少机器非常方便，可以实现自动运维。

高性能。无论是针对整个集群还是单台服务器，都要求分布式存储系统具备高性能。

易用。分布式存储系统需要能够提供易用的对外接口，另外，也要求具备完善的监控、运维工具，并能够方便地与其他系统集成。

### 分布式存储分类

分布式存储面临的数据需求比较复杂，大致可以分为三类：

非机构化数据：包括所有格式的办公文档、文本、图片、图像、音频和视频信息等。结构化数据：一般存储在关系数据库中，可以用二维关系表结构来表示。

结构化数据的模式（Schema，包括属性、数据类型以及数据之间的联系）和内容是分开的，数据的模式需要预先定义。

半结构化数据：介于非结构化数据和结构化数据之间，HTML文档就属于半结构化数据。它一般是自描述的，与结构化数据最大的区别在于，半结构化数据的模式结构和内容混在一起，没有明显的区分，也不需要预先定义数据的模式结构。

同的分布式存储系统适合处理不同类型的数据，本书将分布式存储系统分为四类：分布式文件系统、分布式键值（Key-Value）系统、分布式表格系统和分布式数据库。

#### 分布式文件系统

互联网应用需要存储大量的图片、照片、视频等非结构化数据对象，这类数据以对象的形式组织，对象之间没有关联，这样的数据一般称为Blob（Binary Large Object，二进制大对象）数据。

分布式文件系统用于存储Blob对象，典型的系统有Facebook Haystack以及Taobao FileSystem（TFS）。另外，分布式文件系统也常作为分布式表格系统以及分布式数据库的底层存储，如谷歌的GFS（Google File System，存储大文件）可以作为分布式表格系统GoogleBigtable的底层存储，Amazon的EBS（Elastic Block Store，弹性块存储）系统可以作为分布式数据库（Amazon RDS）的底层存储。

总体上看，分布式文件系统存储三种类型的数据：Blob对象、定长块以及大文件。在系统实现层面，分布式文件系统内部按照数据块（chunk）来组织数据，每个数据块的大小大致相同，每个数据块可以包含多个Blob对象或者定长块，一个大文件也可以拆分为多个数据块，如图1-1所示。分布式文件系统将这些数据块分散到存储集群，处理数据复制、一致性、负载均衡、容错等分布式系统难题，并将用户对Blob对象、定长块以及大文件的操作映射为对底层数据块的操作。

![image-20200405064810336](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405064810336.png)

#### 分布式键值系统

分布式键值系统用于存储关系简单的半结构化数据，它只提供基于主键的CRUD（Create/Read/Update/Delete）功能，即根据主键创建、读取、更新或者删除一条键值记录。

典型的系统有Amazon Dynamo以及Taobao Tair。从数据结构的角度看，分布式键值系统与传统的哈希表比较类似，不同的是，分布式键值系统支持将数据分布到集群中的多个存储节点。分布式键值系统是分布式表格系统的一种简化实现，一般用作缓存，比如淘宝Tair以及Memcache。一致性哈希是分布式键值系统中常用的数据分布技术，因其被AmazonDynamoDB系统使用而变得相当有名。

#### 分布式表格系统

分布式表格系统用于存储关系较为复杂的半结构化数据，与分布式键值系统相比，分布式表格系统不仅仅支持简单的CRUD操作，而且支持扫描某个主键范围。分布式表格系统以表格为单位组织数据，每个表格包括很多行，通过主键标识一行，支持根据主键的CRUD功能以及范围查找功能。

分布式表格系统借鉴了很多关系数据库的技术，例如支持某种程度上的事务，比如单行事务，某个实体组（Entity Group，一个用户下的所有数据往往构成一个实体组）下的多行事务。典型的系统包括Google Bigtable以及Megastore，Microsoft Azure Table Storage，AmazonDynamoDB等。与分布式数据库相比，分布式表格系统主要支持针对单张表格的操作，不支持一些特别复杂的操作，比如多表关联，多表联接，嵌套子查询；另外，在分布式表格系统中，同一个表格的多个数据行也不要求包含相同类型的列，适合半结构化数据。分布式表格系统是一种很好的权衡，这类系统可以做到超大规模，而且支持较多的功能，但实现往往比较复杂，而且有一定的使用门槛。

#### 分布式数据库

分布式数据库一般是从单机关系数据库扩展而来，用于存储结构化数据。分布式数据库采用二维表格组织数据，提供SQL关系查询语言，支持多表关联，嵌套子查询等复杂操作，并提供数据库事务以及并发控制。

典型的系统包括MySQL数据库分片（MySQL Sharding）集群，Amazon RDS以及MicrosoftSQL Azure。分布式数据库支持的功能最为丰富，符合用户使用习惯，但可扩展性往往受到限制。当然，这一点并不是绝对的。Google Spanner系统是一个支持多数据中心的分布式数据库，它不仅支持丰富的关系数据库功能，还能扩展到多个数据中心的成千上万台机器。除此之外，阿里巴巴OceanBase系统也是一个支持自动扩展的分布式关系数据库。

### 分布式文件系统

#### Google文件系统

Google文件系统（GFS）是构建在廉价服务器之上的大型分布式系统。它将服务器故障视为正常现象，通过软件的方式自动容错，在保证系统可靠性和可用性的同时，大大降低系统的成本。

GFS是Google分布式存储的基石，其他存储系统，如Google Bigtable、Google Megastore、Google Percolator均直接或者间接地构建在GFS之上。另外，Google大规模批处理系统MapReduce也需要利用GFS作为海量数据的输入输出。

**1 系统架构**

如图4-1所示，GFS系统的节点可分为三种角色：GFS Master（主控服务器）、GFSChunkServer（CS，数据块服务器）以及GFS客户端。

GFS文件被划分为固定大小的数据块（chunk），由主服务器在创建时分配一个64位全局唯一的chunk句柄。CS以普通的Linux文件的形式将chunk存储在磁盘中。为了保证可靠性，chunk在不同的机器中复制多份，默认为三份。

主控服务器中维护了系统的元数据，包括文件及chunk命名空间、文件到chunk之间的映射、chunk位置信息。它也负责整个系统的全局控制，如chunk租约管理、垃圾回收无用chunk、chunk复制等。主控服务器会定期与CS通过心跳的方式交换信息。

![image-20200405065123443](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405065123443.png)

**2 关键问题**

1.租约机制

GFS数据追加以记录为单位，每个记录的大小为几十KB到几MB不等，如果每次记录追加都需要请求Master，那么Master显然会成为系统的性能瓶颈，因此，GFS系统中通过租约（lease）机制将chunk写操作授权给ChunkServer。拥有租约授权的ChunkServe称为主ChunkServer，其他副本所在的ChunkServer称为备ChunkServer。租约授权针对单个chunk，在租约有效期内，对该chunk的写操作都由主ChunkServer负责，从而减轻Master的负载。一般来说，租约的有效期比较长，比如60秒，只要没有出现异常，主ChunkServer可以不断向Master请求延长租约的有效期直到整个chunk写满。

2.一致性模型

GFS主要是为了追加（append）而不是改写（overwrite）而设计的。一方面是因为改写的需求比较少，或者可以通过追加来实现，比如可以只使用GFS的追加功能构建分布式表格系统Bigtable；另一方面是因为追加的一致性模型相比改写要更加简单有效。考虑chunk A的三个副本A1、A2、A3，有一个改写操作修改了A1、A2但没有修改A3，这样，落到副本A3的读操作可能读到不正确的数据；相应地，如果有一个追加操作往A1、A2上追加了一个记录，但是追加A3失败，那么即使读操作落到副本A3也只是读到过期而不是错误的数据。

3.追加流程

追加流程是GFS系统中最为复杂的地方，而且，高效支持记录追加对基于GFS实现的分布式表格系统Bigtable是至关重要的。如图4-2所示，追加流程大致如下：

![image-20200405065258218](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405065258218.png)

4.容错机制

GFS采用复制多个副本的方式实现ChunkServer的容错，每个chunk有多个存储副本，分别存储在不同的ChunkServer上。对于每个chunk，必须将所有的副本全部写入成功，才视为成功写入。如果相关的副本出现丢失或不可恢复的情况，Master自动将副本复制到其他ChunkServer，从而确保副本保持一定的个数。

**3 Master设计**

1.Master内存占用

Master维护了系统中的元数据，包括文件及chunk命名空间、文件到chunk之间的映射、chunk副本的位置信息。其中前两种元数据需要持久化到磁盘，chunk副本的位置信息不需要持久化，可以通过ChunkServer汇报获取。

2.负载均衡

系统中需要创建chunk副本的情况有三种：chunk创建、chunk复制（re-replication）以及负载均衡（rebalancing）。

当Master创建了一个chunk，它会根据如下因素来选择chunk副本的初始位置：1）新副本所在的ChunkServer的磁盘利用率低于平均水平；2）限制每个Chunk-Server“最近”创建的数量；3）每个chunk的所有副本不能在同一个机架。第二点容易忽略但却很重要，因为创建完chunk以后通常需要马上写入数据，如果不限制“最近”创建的数量，当一台空的ChunkServer上线时，由于磁盘利用率低，可能导致大量的chunk瞬间迁移到这台机器从而将它压垮。

当chunk的副本数量小于一定的数量后，Master会尝试重新复制一个chunk副本。可能的原因包括ChunkServer宕机或者ChunkServer报告自己的副本损坏，或者ChunkServer的某个磁盘故障，或者用户动态增加了chunk的副本数，等等。每个chunk复制任务都有一个优先级，按照优先级从高到低在Master排队等待执行。例如，只有一个副本的chunk需要优先复制。另外，GFS会提高所有阻塞客户端操作的chunk复制任务的优先级，例如客户端正在往一个只有一个副本的chunk追加数据，如果限制至少需要追加成功两个副本，那么这个chunk复制任务会阻塞客户端写操作，需要提高优先级。

3.垃圾回收

GFS采用延迟删除的机制，也就是说，当删除文件后，GFS并不要求立即归还可用的物理存储，而是在元数据中将文件改名为一个隐藏的名字，并且包含一个删除时间戳。Master定时检查，如果发现文件删除超过一段时间（默认为3天，可配置），那么它会把文件从内存元数据中删除，以后ChunkServer和Master的心跳消息中，每一个ChunkServer都将报告自己的chunk集合，Master会回复在Master元数据中已经不存在的chunk信息，这时，ChunkServer会释放这些chunk副本。为了减轻系统的负载，垃圾回收一般在服务低峰期执行，比如每天晚上凌晨1:00开始。

4.快照

快照（Snapshot）操作是对源文件/目录进行一个“快照”操作，生成该时刻源文件/目录的一个瞬间状态存放于目标文件/目录中。GFS中使用标准的写时复制机制生成快照，也就是说，“快照”只是增加GFS中chunk的引用计数，表示这个chunk被快照文件引用了，等到客户端修改这个chunk时，才需要在ChunkServer中拷贝chunk的数据生成新的chunk，后续的修改操作落到新生成的chunk上。

####  Taobao File System

2007年以前淘宝的图片存储系统使用了昂贵的NetApp存储设备，由于淘宝数据量大且增长很快，出于性能和成本的考虑，淘宝自主研发了Blob存储系统Tabao File System（TFS）。目前，TFS中存储的图片规模已经达到百亿级别。

TFS架构设计时需要考虑如下两个问题：

Metadata信息存储。由于图片数量巨大，单机存放不了所有的元数据信息，假设每个图片文件的元数据占用100字节，100亿图片的元数据占用的空间为10G×0.1KB=1TB，单台机器无法提供元数据服务。

减少图片读取的IO次数。在普通的Linux文件系统中，读取一个文件包括三次磁盘IO：首先读取目录元数据到内存，其次把文件的inode节点装载到内存，最后读取实际的文件内容。由于小文件个数太多，无法将所有目录及文件的inode信息缓存到内存，因此磁盘IO次数很难达到每个图片读取只需要一次磁盘IO的理想状态。因此，TFS设计时采用的思路是：多个逻辑图片文件共享一个物理文件。

**1 系统架构**

TFS架构上借鉴了GFS，但与GFS又有很大的不同。首先，TFS内部不维护文件目录树，每个小文件使用一个64位的编号表示；其次，TFS是一个读多写少的应用，相比GFS，TFS的写流程可以做得更加简单有效。

如图4-4所示，一个TFS集群由两个NameServer节点（一主一备）和多个DataServer节点组成，NameServer通过心跳对DataSrver的状态进行监测。NameServer相当于GFS中的Master，DataServer相当于GFS中的ChunkServer。NameServer区分为主NameServer和备NameServer，只有主NameServer提供服务，当主NameServer出现故障时，能够被心跳守护进程检测到，并将服务切换到备NameServer。每个DataServer上会运行多个dsp进程，一个dsp对应一个挂载点，这个挂载点一般对应一个独立磁盘，从而管理多块磁盘。

![image-20200405065615513](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405065615513.png)

在TFS中，将大量的小文件（实际数据文件）合并成一个大文件，这个大文件称为块（Block），每个Block拥有在集群内唯一的编号（块ID），通过<块ID，块内偏移>可以唯一确定一个文件。TFS中Block的实际数据都存储在DataServer中，大小一般为64MB，默认存储三份，相当于GFS中的chunk。应用客户端是TFS提供给应用程序的访问接口，应用客户端不缓存文件数据，只缓存NameServer的元数据。

#### Facebook Haystack

Facebook目前存储了2600亿张照片，总大小为20PB，通过计算可以得出每张照片的平均大小为20PB/260GB，约为80KB。用户每周新增照片数为10亿（总大小为60TB），平均每秒新增的照片数为109/7/40000（按每天40000s计），约为每秒3500次写操作，读操作峰值可以达到每秒百万次。

Facebook相册后端早期采用基于NAS的存储，通过NFS挂载NAS中的照片文件来提供服务。后来出于性能和成本考虑，自主研发了Facebook Haystack存储相册数据。

**1 系统架构**

Facebook Haystack的思路与TFS类似，也是多个逻辑文件共享一个物理文件。Haystack架构及读请求处理流程如图4-6所示。

![image-20200405065700367](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405065700367.png)

Haystack系统主要包括三个部分：目录（Directory）、存储（Store）以及缓存（Cache）。Haystack存储是物理存储节点，以物理卷轴（physical volume）的形式组织存储空间，每个物理卷轴一般都很大，比如100GB，这样10TB的数据也只需100个物理卷轴。每个物理卷轴对应一个物理文件，因此，每个存储节点上的物理文件元数据都很小。多个物理存储节点上的物理卷轴组成一个逻辑卷轴（logical volume），用于备份。Haystack目录存放逻辑卷轴和物理卷轴的对应关系，以及照片id到逻辑卷轴之间的映射关系。Haystack缓存主要用于解决对CDN提供商过于依赖的问题，提供最近增加的照片的缓存服务。

### 分布式键值系统

分布式键值模型可以看成是分布式表格模型的一种特例。然而，由于它只支持针对单个key-value的增、删、查、改操作，因此，适用3.3.1节提到的哈希分布算法。

Amazon Dynamo是分布式键值系统，最初用于支持购物车应用。Dynamo将很多分布式技术融合到一个系统内，学习Dynamo的设计对理解分布式系统的理论很有帮助。当然，这个系统的主要价值在于学术层面，从工程的角度看，Dynamo牺牲了一致性，却没有换来什么好处，不适合直接模仿。

Tair是淘宝网开发的分布式键值系统，它借鉴了Dynamo系统的一些设计思路并做了一些创新，其中最大的变化就是从P2P架构修改为带有中心节点的架构，笔者认为，这种思路在大方向上是正确的。

#### Amazon Dynamo

Dynamo以很简单的键值方式存储数据，不支持复杂的查询。Dynamo中存储的是数据值的原始形式，不解析数据的具体内容。Dynamo主要用于Amazon的购物车及S3云存储服务。

Dynamo通过组合P2P的各种技术打造了线上可运行的分布式键值系统，表5-1中列出了Dynamo设计时面临的问题及最终采取的解决方案。

![image-20200405065811544](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405065811544.png)

**1 数据分布**

Dynamo系统采用一致性哈希算法将数据分布到多个存储节点中。一致性哈希算法思想如下：给系统中每个节点分配一个随机token，这些token构成一个哈希环。执行数据存放操作时，先计算主键的哈希值，然后存放到顺时针方向第一个大于或者等于该哈希值的token所在的节点。一致性哈希的优点在于节点加入/删除时只会影响到在哈希环中相邻的节点，而对其他节点没影响。

**2 一致性与复制**

为了处理节点失效的情况（DHT环中删除节点），需要对节点的数据进行复制。思路如下：假设数据存储N份，DHT定位到的数据所属节点为K，则数据存储在节点K，K+1，...，K+N-1上。如果第K+i（0≤i≤N-1）台机器宕机，则往后找一台机器K+N临时替代。如果第K+i台机器重启，临时替代的机器K+N能够通过Gossip协议发现，它会将这些临时数据归还K+i，这个过程在Dynamo中叫做数据回传（Hinted Handoff）。机器K+i宕机的这段时间内，所有的读写均落入到机器[K，K+i-1]和[K+i+1，K+N]中。如果机器K+i永久失效，机器K+N需要进行数据同步操作。一般来说，从机器K+i宕机开始到被认定为永久失效的时间不会太长，积累的写操作也不会太多，可以利用Merkle树对机器的数据文件进行快速同步。

**3 容错**

Dynamo把异常分为两种类型：临时性的异常和永久性异常。有一些异常是临时性的，比如机器假死；其他异常，如硬盘报修或机器报废等，由于其持续时间太长，称为永久性的。下面解释Dynamo的容错机制：

数据回传 

在Dynamo设计中，一份数据被写到K，K+1，...，K+N-1这N台机器上，如果机器K+i（0≤i≤N-1）宕机，原本写入该机器的数据转移到机器K+N，如果在指定的时间T内K+i重新提供服务，机器K+N将通过Gossip协议发现，并将启动传输任务将暂存的数据回传给机器K+i。

Merkle树同步 

如果超过了时间T机器K+i还是处于宕机状态，这种异常被认为是永久性的。这时需要借助Merkle树机制从其他副本进行数据同步。Merkle树同步的原理很简单，每个非叶子节点对应多个文件，为其所有子节点值组合以后的哈希值；叶子节点对应单个数据文件，为文件内容的哈希值。这样，任何一个数据文件不匹配都将导致从该文件对应的叶子节点到根节点的所有节点值不同。每台机器对每一段范围的数据维护一颗Merkle树，机器同步时首先传输Merkle树信息，并且只需要同步从根到叶子的所有节点值均不相同的文件。

读取修复

 假设N=3，W=2，R=2，机器K宕机，可能有部分写操作已经返回客户端成功了但是没有完全同步到所有的副本，如果机器K出现永久性异常，比如磁盘故障，三个副本之间的数据一直都不一致。客户端的读取操作如果发现了某些副本版本太老，则启动异步的读取修复任务。该任务会合并多个副本的数据，并使用合并后的结果更新过期的副本，从而使得副本之间保持一致。

**4 负载均衡**

Dynamo的负载均衡取决于如何给每台机器分配虚拟节点号，即token。由于集群环境的异构性，每台物理机器包含多个虚拟节点。一般有如下两种分配节点号的方法。

**随机分配。**

每台物理节点加入时根据其配置情况随机分配S个Token。这种方法的负载平衡效果还是不错的，因为自然界的数据大致是比较随机的，虽然可能出现某段范围的数据特别多的情况（如baidu、sina等域名下的网页特别多），但是只要切分足够细，即S足够大，负载还是比较均衡的。这个方法的问题是可控性较差，新节点加入/离开系统时，集群中的原有节点都需要扫描所有的数据从而找出属于新节点的数据，Merkle树也需要全部更新；另外，增量归档/备份变得几乎不可能。

**数据范围等分+随机分配。**

为了解决上种方法的问题，首先将数据的哈希空间等分为Q=N×S份（N=机器个数，S=每台机器的虚拟节点数），然后每台机器随机选择S个分割点作为Token。和上种方法一样，这种方法的负载也比较均衡，并且每台机器都可以对属于每个范围的数据维护一颗逻辑上的Merkle树，新节点加入/离开时只需扫描部分数据进行同步，并更新这部分数据对应的逻辑Merkle树，增量归档也变得简单。

另外，Dynamo对单机的前后台任务资源分配也做了一些工作。Dynamo中同步操作、写操作重试等后台任务较多。为了不影响正常的读写服务，需要对后台任务能够使用的资源做出限制。Dynamo中维护一个资源授权系统。该系统将整个机器的资源切分成多个片，监控60秒内的磁盘读写响应时间，事务超时时间及锁冲突情况，根据监控信息算出机器负载从而动态调整分配给后台任务的资源片个数。

#### 淘宝Tair

Tair是淘宝开发的一个分布式键/值存储引擎。Tair分为持久化和非持久化两种使用方式：非持久化的Tair可以看成是一个分布式缓存，持久化的Tair将数据存放于磁盘中。为了解决磁盘损坏导致数据丢失，Tair可以配置数据的备份数目，Tair自动将一份数据的不同备份放到不同的节点上，当有节点发生异常，无法正常提供服务的时候，其余的节点会继续提供服务。

**1 系统架构**

Tair作为一个分布式系统，是由一个中心控制节点和若干个服务节点组成。其中，中心控制节点称为Config Server，服务节点称为Data Server。Config Server负责管理所有的Data Server，维护其状态信息；Data Server对外提供各种数据服务，并以心跳的形式将自身状况汇报给Config Server。Config Server是控制点，而且是单点，目前采用一主一备的形式来保证可靠性，所有的Data Server地位都是等价的。

图5-5是Tair的系统架构图。客户端首先请求Config Server获取数据所在的Data Server，接着往Data Server发送读写请求。Tair允许将数据存放到多台Data Server，以实现异常容错。

![image-20200405070202333](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405070202333.png)

**2 关键问题**

（1）数据分布

根据数据的主键计算哈希值后，分布到Q个桶中，桶是负载均衡和数据迁移的基本单位。ConfigServer按照一定的策略把每个桶指派到不同的Data Server上。因为数据按照主键计算哈希值，所以可以认为每个桶中的数据基本是平衡的，只要保证桶分布的均衡性，就能够保证数据分布的均衡性。根据Dynamo论文中的实验结论，Q取值需要远大于集群的物理机器数，例如Q取值10240。

（2）容错

当某台Data Server故障不可用时，Config Server能够检测到。每个哈希桶在Tair中存储多个副本，如果是备副本，那么Config Server会重新为其指定一台Data Server，如果是持久化存储，还将复制数据到新的Data Server上。如果是主副本，那么Config Server首先将某个正常的备副本提升为主副本，对外提供服务。接着，再选择另外一台Data Server增加一个备副本，确保数据的备份数。

（3）数据迁移

机器加入或者负载不均衡可能导致桶迁移，迁移的过程中需要保证对外服务。当迁移发生时，假设Data Server A要把桶3、4、5迁移到Data Server B。迁移完成前，客户端的路由表没有变化，客户端对3、4、5的访问请求都会路由到A。现在假设3还没开始迁移，4正在迁移中，5已经迁移完成。那么如果对3访问，A直接服务；如果对5访问，A会把请求转发给B，并且将B的返回结果返回给用户；如果对4访问，由A处理，同时如果是对4的修改操作，会记录修改日志，等到桶4迁移完成时，还要把修改日志发送到B，在B上应用这些修改操作，直到A和B之间数据完全一致迁移才真正完成。

（4）Config Server

客户端缓存路由表，大多数情况下，客户端不需要访问Config Server，Config Server宕机也不影响客户端正常访问。每次路由的变更，Config Server都会将新的配置信息推给Data Server。在客户端访问Data Server的时候，会发送客户端缓存的路由表的版本号。如果Data Server发现客户端的版本号过旧，则会通知客户端去Config Server获取一份新的路由表。如果客户端访问某台Data Server发生了不可达的情况（该Data Server可能宕机了），客户端会主动去ConfigServer获取新的路由表。

（5）Data Server

Data Server负责数据的存储，并根据Config Server的要求完成数据的复制和迁移工作。DataServer具备抽象的存储引擎层，可以很方便地添加新存储引擎。Data Server还有一个插件容器，可以动态加载/卸载插件。

### 分布式表格系统

分布式表格系统对外提供表格模型，每个表格由很多行组成，通过主键唯一标识，每一行包含很多列。

#### Google Bigtable

Bigtable是Google开发的基于GFS和Chubby的分布式表格系统。Google的很多数据，包括Web索引、卫星图像数据等在内的海量结构化和半结构化数据，都存储在Bigtable中。与Google的其他系统一样，Bigtable的设计理念是构建在廉价的硬件之上，通过软件层面提供自动化容错和线性可扩展性能力。

**1 架构**

Bigtable构建在GFS之上，为文件系统增加一层分布式索引层。另外，Bigtable依赖Google的Chubby（即分布式锁服务）进行服务器选举及全局信息维护。

如图6-2所示，Bigtable将大表划分为大小在100～200MB的子表（tablet），每个子表对应一个连续的数据范围。Bigtable主要由三个部分组成：客户端程序库（Client）、一个主控服务器（Master）和多个子表服务器（Tablet Server）。

客户端程序库（Client）：提供Bigtable到应用程序的接口，应用程序通过客户端程序库对表格的数据单元进行增、删、查、改等操作。客户端通过Chubby锁服务获取一些控制信息，但所有表格的数据内容都在客户端与子表服务器之间直接传送；主控服务器（Master）：管理所有的子表服务器，包括分配子表给子表服务器，指导子表服务器实现子表的合并，接受来自子表服务器的子表分裂消息，监控子表服务器，在子表服务器之间进行负载均衡并实现子表服务器的故障恢复等。子表服务器（Tablet Server）：实现子表的装载/卸出、表格内容的读和写，子表的合并和分裂。Tablet Server服务的数据包括操作日志以及每个子表上的sstable数据，这些数据存储在底层的GFS中。

![image-20200405070432359](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405070432359.png)

Bigtable依赖于Chubby锁服务完成如下功能：

1）选取并保证同一时间内只有一个主控服务器；

2）存储Bigtable系统引导信息；

3）用于配合主控服务器发现子表服务器加入和下线；

4）获取Bigtable表格的schema信息及访问控制信息。

Chubby是一个分布式锁服务，底层的核心算法为Paxos。Paxos算法的实现过程需要一个“多数派”就某个值达成一致，进而才能得到一个分布式一致性状态。也就是说，只要一半以上的节点不发生故障，Chubby就能够正常提供服务。Chubby服务部署在多个数据中心，典型的部署为两地三数据中心五副本，同城的两个数据中心分别部署两个副本，异地的数据中心部署一个副本，任何一个数据中心整体发生故障都不影响正常服务。

**2 数据分布**

Bigtable中的数据在系统中切分为大小100～200MB的子表，所有的数据按照行主键全局排序。Bigtable中包含两级元数据，元数据表及根表。用户表在进行某些操作，比如子表分裂的时候需要修改元数据表，元数据表的某些操作又需要修改根表。通过使用两级元数据，提高了系统能够支持的数据量。假设平均一个子表大小为128MB，每个子表的元信息为1KB，那么一级元数据能够支持的数据量为128MB×（128MB/1KB）=16TB，两级元数据能够支持的数据量为16TB×（128MB/1KB）=2048PB，满足几乎所有业务的数据量需求。如图6-3所示。

![image-20200405070516954](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405070516954.png)

**3 复制与一致性**

Bigtable系统保证强一致性，同一个时刻同一个子表只能被一台Tablet Server服务，也就是说，Master将子表分配给某个Tablet Server服务时需要确保没有其他的Tablet Server正在服务这个子表。这是通过Chubby的互斥锁机制保证的，Tablet Server启动时需要获取Chubby互斥锁，当Tablet Server出现故障，Master需要等到Tablet Server的互斥锁失效，才能把它上面的子表迁移到其他Tablet Server。

Bigtable写入GFS的数据分为两种：

操作日志。当Tablet Server发生故障时，它上面服务的子表会被集群中的其他Tablet Server加载继续提供服务。加载子表可能需要回放操作日志，每条操作日志都有唯一的序号，通过它可以去除重复的操作日志。每个子表包含的SSTable数据。如果写入GFS失败可以重试并产生多条重复记录，但是Bigtable只会索引最后一条写入成功的记录。

Bigtable本质上是构建在GFS之上的一层分布式索引，通过它解决了GFS遗留的一致性问题，大大简化了用户使用。

#### Google Megastore

Google Bigtable架构把可扩展性基本做到了极致，Megastore则是在Bigtable系统之上提供友好的数据库功能支持，增强易用性。Megastore是介于传统的关系型数据库和NoSQL之间的存储技术，它在Google内部使用广泛，如Google App Engine、社交类应用等。

**1 系统架构**

如图6-6所示，Megastore系统由三个部分组成：

客户端库：提供Megastore到应用程序的接口，应用程序通过客户端操作Megastore的实体组。Megastore系统大部分功能集中在客户端，包括映射Megastore操作到Bigtable，事务及并发控制，基于Paxos的复制，将请求分送给复制服务器，通过协调者实现快速读等。

复制服务器：接受客户端的用户请求并转发到所在机房的Bigtable实例，用于解决跨机房连接数过多的问题。

协调者：存储每个机房本地的实体组是否处于最新状态的信息，用于实现快速读。

Megastore的功能主要分为三个部分：映射Megastore数据模型到Bigtable，事务及并发控制，跨机房数据复制及读写优化。Megastore首先解析用户通过客户端传入的SQL请求，接着根据用户定义的Megastore数据模型将SQL请求转化为对底层Bigtable的操作。

![image-20200405070655376](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405070655376.png)

#### Windows Azure Storage

Windows Azure Storage（WAS）是微软开发的云存储系统，包括三种数据存储服务：Windows Azure Blob、Windows Azure Table、Windows Azure Queue。三种数据存储服务共享一套底层架构，在微软内部广泛用于社会化网络、视频、游戏、Bing搜索等业务。另外，在微软外部也有成千上万个云存储客户。

**1 整体架构**

WAS部署在不同地域的多个数据中心，依赖底层的Windows Azure结构控制器（FabricController）管理硬件资源。结构控制器的功能包括节点管理，网络配置，健康检查，服务启动，关闭，部署和升级。另外，WAS还通过请求结构控制器获取网络拓扑信息，集群物理部署以及存储节点硬件配置信息。

如图6-10所示，WAS主要分为两个部分：定位服务（Location Service，LS）和存储区（Storage Stamp）。

定位服务的功能包括：管理所有的存储区，管理用户到存储区之间的映射关系，收集存储区的负载信息，分配新用户到负载较轻的存储区。LS服务自身也分布在两个不同的地域以实现高可用。LS需要通过DNS服务来使得每个账户的请求定位到所属存储区。

每个存储区是一个集群，一般由10～20个机架组成，每个机架有18个存储节点，提供大约2PB存储容量。下一步的计划是扩大存储区规模，使得每个存储区能够容纳30PB原始数据。存储区分为三层：文件流层（Stream Layer）、分区层（Partition Layer）以及前端层（Front-EndLayer）。

文件流层：与Google GFS类似，提供分布式文件存储。WAS中文件称为流（streams），文件中的Chunk称为范围（extent）。文件流层一般不直接对外服务，需要通过服务分区层访问。分区层：与Google Bigtable类似，将对象划分到不同的分区以被不同的分区服务器（PartitionServer）服务，分区服务器将对象持久化到文件流层。

前端层：前端层包括一系列无状态的Web服务器，这些Web服务器完成权限验证等功能并根据请求的分区名（Partition Name）将请求转发到不同的分区服务器。分区映射表（PartitionMap）用来决定应该将请求转化到哪个分区服务器，前端服务器一般缓存了此表从而减少一次网络请求。

![image-20200405070801173](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405070801173.png)

另外，WAS包含两种复制方式：

存储区内复制（Intra-Stamp Replication）：文件流层实现，同一个extent的多个副本之间的复制模式为强同步，每个成功的写操作必须保证所有副本都同步成功，用来实现强一致性。

跨存储区复制（Inter-Stamp Replication）：服务分区层实现，通过后台线程异步复制到不同的存储区，用来实现异地容灾。

### 分布式数据库

#### 数据库中间层

为了扩展关系数据库，最简单也是最为常见的做法就是应用层按照规则将数据拆分为多个分片，分布到多个数据库节点，并引入一个中间层来对应用屏蔽后端的数据库拆分细节。

**1 架构**

以MySQL Sharding架构为例，分为几个部分：中间层dbproxy集群、数据库组、元数据服务器、常驻进程，如图7-1所示。

（1）MySQL客户端库

应用程序通过MySQL原生的客户端与系统交互，支持JDBC，原有的单机访问数据库程序可以无缝迁移。

（2）中间层dbproxy

中间层解析客户端SQL请求并转发到后端的数据库。具体来讲，它解析MySQL协议，执行SQL路由，SQL过滤，读写分离，结果归并，排序以及分组，等等。中间层由多个无状态的dbproxy进程组成，不存在单点的情况。另外，可以在客户端与中间层之间引入LVS（Linux VirtualServer）对客户端请求进行负载均衡。需要注意的是，引入LVS后，客户端请求需要额外增加一层通信开销，因此，常见的做法是直接在客户端配置中间层服务器列表，由客户端处理请求负载均衡以及中间层服务器故障等情况。

![image-20200405070955826](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405070955826.png)

（3）数据库组dbgroup

每个dbgroup由N台数据库机器组成，其中一台为主机（Master），另外N-1台为备机（Slave）。主机负责所有的写事务及强一致读事务，并将操作以binlog的形式复制到备机，备机可以支持有一定延迟的读事务。

（4）元数据服务器

元数据服务器主要负责维护dbgroup拆分规则并用于dbgroup选主。dbproxy通过元数据服务器获取拆分规则从而确定SQL语句的执行计划。另外，如果dbgroup的主机出现故障，需要通过元数据服务器选主。元数据服务器本身也需要多个副本实现HA，一种常见的方式是采用Zookeeper实现。

（5）常驻进程agents

部署在每台数据库服务器上的常驻进程，用于实现监控，单点切换，安装，卸载程序等。dbgroup中的数据库需要进行主备切换，软件升级等，这些控制逻辑需要与数据库读写事务处理逻辑隔离开来。

#### Microsoft SQL Azure

Microsoft SQL Azure是微软的云关系型数据库，后端存储又称为云SQL Server（Cloud SQLServer）。它构建在SQL Server之上，通过分布式技术提升传统关系型数据库的可扩展性和容错能力。

**1.逻辑模型**

云SQL Server将数据划分为多个分区，通过限制事务只能在一个分区执行来规避分布式事务。另外，它通过主备复制（Primary-Copy）协议将数据复制到多个副本，保证高可用性。

云SQL Server中一个逻辑数据库称为一个表格组（table group），它既可以是有主键的，也可以是无主键的，本节只讨论有主键的表格组。如果一个表格组是有主键的，要求表格组中所有的表格都有一个相同的列，称为划分主键（partitioning key）。图中的表格组包含两个表格，顾客表（Customers）和订单表（Orders），划分主键为顾客ID（Customers表中的Id列）。如图7-2所示。

![image-20200405071134795](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405071134795.png)

2 **架构**

云SQL Server分为四个主要部分：SQL Server实例、全局分区管理、协议网关、分布式基础部件，如图7-4所示。下面分别介绍这几个部分：

每个SQL Server实例是一个运行着SQLServer的物理进程。每个物理数据库包含多个子数据库，它们之间互相隔离。子数据库是一个分区，包含用户的数据以及schema信息。

全局分区管理器（Global Partition Mana-ger）维护分区映射表信息，包括每个分区的主键范围，每个副本所在的服务器，以及每个副本的状态，包括副本当前是主还是备，前一次是主还是备，正在变成主，正在被拷贝或者正在被追赶。当服务器发生故障时，分布式基础部件检测并确保服务器故障后通知全局分区管理器。全局分区管理器接着执行重新配置操作。另外，全局分区管理器监控集群中的SQL Server工作机，执行负载均衡，副本拷贝等管理操作。

协议网关（Protocol Gateway）负责将用户的数据库连接请求转发到相应的主分区上。协议网关通过全局分区管理器获取分区所在的SQL Server实例，后续的读写事务操作都在网关与SQLServer实例之间进行。

分布式基础部件（Distributed Fabric）用于维护机器上下线状态，检测服务器故障并为集群中的各种角色执行选举主节点操作。它在每台服务器上都运行了一个守护进程。

![image-20200405071227313](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405071227313.png)

#### Google Spanner

Google Spanner是Google的全球级分布式数据库（Globally-Distributed Database）。Spanner的扩展性达到了全球级，可以扩展到数百个数据中心，数百万台机器，上万亿行记录。更为重要的是，除了夸张的可扩展性之外，它还能通过同步复制和多版本控制来满足外部一致性，支持跨数据中心事务。

### OceanBase

#### OceanBase

OceanBase是阿里集团研发的可扩展的关系数据库，实现了数千亿条记录、数百TB数据上的跨行跨表事务，截止到2012年8月，支持了收藏夹、直通车报表、天猫评价等OLTP和OLAP在线业务，线上数据量已经超过一千亿条。

从模块划分的角度看，OceanBase可以划分为四个模块：主控服务器RootServer、更新服务器UpdateServer、基线数据服务器ChunkServer以及合并服务器MergeServer。OceanBase系统内部按照时间线将数据划分为基线数据和增量数据，基线数据是只读的，所有的修改更新到增量数据中，系统内部通过合并操作定期将增量数据融合到基线数据中。本章介绍OceanBase系统的设计思路和整体架构。

#### 分布式存储引擎

分布式存储引擎层负责处理分布式系统中的各种问题，例如数据分布、负载均衡、容错、一致性协议等。与其他分布式存储系统类似，分布式存储引擎层支持根据主键更新、插入、删除、随机读取以及范围查找等操作，数据库功能层构建在分布式存储引擎层之上。

分布式存储引擎层包含三个模块：RootServer、UpdateServer以及ChunkServer。其中，RootServer用于整体控制，实现子表分布、副本复制、负载均衡、机器管理以及Schema管理；UpdateServer用于存储增量数据，数据结构为一个内存B树，并通过主备实时同步实现高可用，另外，UpdateServer的网络框架也经过专门的优化；ChunkServer用于存储基线数据，基线数据按照主键有序划分为一个个子表，每个子表在ChunkServer上存储了一个或者多个SSTable，另外，定期合并和数据分发的主要逻辑也由ChunkServer实现。

**1 公共模块**

OceanBase源代码中有一个公共模块，包含其他模块需要的公共类，例如公共数据结构、内存管理、锁、任务队列、RPC框架、压缩/解压缩等。

**2 RootServer实现机制**

RootServer是OceanBase集群对外的窗口，客户端通过RootServer获取集群中其他模块的信息。RootServer实现的功能包括：管理集群中的所有ChunkServer，处理ChunkServer上下线；管理集群中的UpdateServer，实现UpdateServer选主；管理集群中子表数据分布，发起子表复制、迁移以及合并等操作；与ChunkServer保持心跳，接受ChunkServer汇报，处理子表分裂；接受UpdateServer汇报的大版本冻结消息，通知ChunkServer执行定期合并；实现主备RootServer，数据强同步，支持主RootServer宕机自动切换。

**3 UpdateServer实现机制**

UpdateServer用于存储增量数据，它是一个单机存储系统，由如下几个部分组成：内存存储引擎，在内存中存储修改增量，支持冻结以及转储操作；任务处理模型，包括网络框架、任务队列、工作线程等，针对小数据包做了专门的优化；主备同步模块，将更新事务以操作日志的形式同步到备UpdateServer。UpdateServer是OceanBase性能瓶颈点，核心是高效，实现时对锁（例如，无锁数据结构）、索引结构、内存占用、任务处理模型以及主备同步都需要做专门的优化。

**4 ChunkServer实现机制**

ChunkServer用于存储基线数据，它由如下基本部分组成：管理子表，主动实现子表分裂，配合RootServer实现子表迁移、删除、合并；SSTable，根据主键有序存储每个子表的基线数据；基于LRU实现块缓存（Block cache）以及行缓存（Row cache）；实现Direct IO，磁盘IO与CPU计算并行化；通过定期合并&数据分发获取UpdateServer的冻结数据，从而分散到整个集群。每台ChunkServer服务着几千到几万个子表的基线数据，每个子表由若干个SSTable组成（一般为1个）。下面从SSTable开始介绍ChunkServer的内部实现。

#### 数据库功能

对于使用者来说，OceanBase与MySQL数据库并没有什么区别，可以通过MySQL客户端连接OceanBase，也可以在程序中通过JDBC/ODBC操作OceanBase。OceanBase的MergeServer模块支持MySQL协议，能够将其中的SQL请求解析出来，并转化为OceanBase系统的内部调用。

OceanBase定位为全功能的关系数据库，但这并不代表我们会同等对待所有的关系数据库功能。关系数据库系统中优化器是最为复杂的，这个问题困扰了关系数据库几十年，更不可能是OceanBase的长项。因此，OceanBase支持的SQL语句一般比较简单，绝大部分为针对单张表格的操作，只有很少一部分操作涉及多张表格。OceanBase内部将事务划分为只读事务和读写事务，只读事务执行过程中不需要加锁，读写事务最终需要发给UpdateServer执行。相比传统的关系数据库，OceanBase执行简单的SQL语句要高效得多。

除了支持OLTP业务，OceanBase还能够支持OLAP业务。OLAP业务的查询请求并发数不会太高，但每次查询的数据量都非常大。为此，OceanBase专门设计了并行计算框架和列式存储来处理OLAP业务面临的大查询问题。

#### 运维及实践

系统的性能和稳定性得到保障后，还需要具备良好的可运维性。OceanBase借鉴了Oracle数据库中的“系统表”机制，将表格Schema、监控数据、系统内部状态等信息保存到内部系统表中，从而能够基于系统表构建监控界面、运维管理界面以及运维工具。

互联网基础产品的质量保证不只是QA的事情，从RD设计、编码开始，系统提测，直至最后上线，每个环节都需要重视质量保证工作。OceanBase的质量保证体系如图11-1所示。

![image-20200405072019205](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200405072019205.png)

## 问题

### 1.多系统之间怎么实现通信的？A系统—》B系统的服务

有两种通信方式，第一种是利用HttpClient，HttpClient提供了http服务的能力，其工作原理就类似于我们去打开浏览器访问一个网页去获取数据，最终网页将数据展现出来。HttpClient可以利用get或者post请求去抓取一个接口的数据，从而得到我们需要的数据。

还有一种便是MQ,使用前，首先搭建一个rabbitMQ的服务器，MQ和HttpClient不同的地方在于HttpClient是同步调用，而MQ可以解耦的异步调用的，正是因为这个原因，MQ才很好的解决了同步的响应速度慢的问题。在这里我们使用的是rabbiMq,同类的产品还有例如ActiveMQ，Kafka等。 什么时候使用异步，什么时候同步？ （比如我们的缓存系统）。

### 2.Solr集群的搭建

服务器的数量：zookeeper：3台服务器
solr：4台服务器
先搭建zookeeper集群，因为zookeeper集群有存活过半机制，一般服务器选用奇数台最少3台，因为一台就不叫集群了叫zookeeper服务器了，一个leader主节点，两个follower节点
搭建完zookeeper集群，启动zookeeper,启动4台tomcat实例，更改tomcat端口号一般改为8080,8081,8082,8083，再搭建4个单机版solr实例，让zookeeper集群集中管理配置文件，将配置文件上传到zookeeper，将conf下面的内容上传到zookeeper集群中，修改solr.xml的文件，告诉每个solr实例zookeeper集群的位置，在每台Tomcat的bin目录下catalina.bat文件中加入DzkHost指定的zookeeper服务器地址。

### 3.请你谈谈对MQ的理解？以及你们在项目中是怎么用的？

MQ（消息队列）是一种应用程序对应应用程序的通信方法，由于在高并发环境下，由于来不及同步处理，请求往往发生堵塞，通过消息队列，我们可以异步处理请求，缓解系统压力；MQ（ Message Queue） ,即消息队列是在消息的传输过程中保存消息的容器。
通俗的说， 就是一个容器， 你把消息丢进去， 不需要立即处理。 然后有个程序去从你的容器里面把消息一条条读出来处理。一般用于应用系统解耦、 消息异步分发， 能够提高系统吞吐量。

消息队列
注册用户，发邮件（异步）
登录，发短信通知（异步），加积分（异步）
商品添加，异步更新solr，异步更新静态页面

静态页面—库存—实时性较差
接口，库存修改后,重新生成新的静态页面

### 4.请你谈谈单点登录的实现方案？你们怎么包括cookie的安全性？跨域取cookie的问题，你们怎么解决的？

单点登录使用了Redis+Cookie实现
把用户信息放在Redis中,Key作为用户凭证存放在Cookie中放在客户端,通过获取Cookie凭证判断用户是否有登录
Cookie的安全性,我们的凭证是唯一的UUID,使用工具类统一字符串命名,并且设置了Cookie,关闭document.cookie的取值功能
Cookie的跨域问题,在二级域名使用共享Cookie的将多个系统的域名统一作为二级域名,统一平台提供使用主域名,cookie.setPath("/")设置Cooie路径为根路径,通过cookie.setDomain(".父域名")使得项目之间跨域互相访问他们的Cookie。

### 5.请你谈谈购物车的实现方案？当商品信息发生变更，购物车中的商品信息是否可以同步到变化？

现实中购物车有两种情况，未登录时的购物车和登录时的购物车。我们用Redis+Cookie的方法来实现购物车。当点击“加入购物车”按钮时，先获取用户登录凭证，如果没有登录,就将商品的id保存在Redis未登录购物车中，当拦截器拦截到用户登录时，把购物车的内容合并到数据库中登录后购物车里，通过json解析商品id查到商品信息，所以购物车中的商品信息是可以变化的。

### 6.如何应对高并发问题？

1.HTML静态化，消耗最小的纯静态化的html页面避免大量的数据库访问请求
2.分离图片服务器,对于web服务器来说,图片是最消耗资源的将图片资源和页面资源进行分离,进行不同的配置优化,保证更改的系统消耗和执行效率
3.数据库集群和库表散列,数据库集群由于在架构、成本、扩张性方面都会受到所采用的关系型的限制，在应用程序安装业务和功能模块将数据库进行分离，不同的模块对应不同的数据库或者表，再进行更小的数据库散列，最终可以再配置让系统随时增加数据库补充系统性能；
4.缓存，使用外加的redis模块进行缓存，减轻数据库访问压力
5.负载均衡，在服务器集群中需一台服务器调度角色Nginx，用户所有请求先由它接收，在分配某台服务器去处理；实现负载均衡：http重定向实现，DNS匹配，反向代理
6.动静态分离,对于动态请求交给Tomcat而其他静态请求,搭建专门的静态资源服务器,使用nginx进行请求分发。

### 7 一致性Hash算法

#### 1.hash算法

那么什么是hash算法呢，百度百科的定义如下：

哈希算法将任意长度的二进制值映射为较短的固定长度的二进制值，这个小的二进制值称为哈希值。哈希值是一段数据唯一且极其紧凑的数值表示形式。

**普通的hash算法在分布式应用中的不足：**

比如，在分布式的存储系统中，要将数据存储到具体的节点上，如果我们采用普通的hash算法进行路由，将数据映射到具体的节点上，如key%N，key是数据的key，N是机器节点数，如果有一个机器加入或退出这个集群，则所有的数据映射都无效了，如果是持久化存储则要做数据迁移，如果是分布式缓存，则其他缓存就失效了。

接下来我们来了解，一致性hash算法是怎么解决这个问题的。

#### 2.一致性hash算法

一致性哈希提出了在动态变化的Cache环境中，哈希算法应该满足的4个适应条件(from 百度百科)：

均衡性(Balance)

平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。

单调性(Monotonicity)

单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。（这段翻译信息有负面价值的，当缓冲区大小变化时一致性哈希(Consistent hashing)尽量保护已分配的内容不会被重新映射到新缓冲区。）

分散性(Spread)

在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。

负载(Load)

负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

接下来说一下具体的设计：

#### 2.1环形hash空间

按照常用的hash算法来将对应的key哈希到一个具有2^32次方个节点的空间中，即0 ~ (2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。

**NOTE:**当然，节点的个数可以自定义，整个hash环我们可以用TreeMap来实现，因为treeMap是排序的，我们刚好可以利用上。

![img](https://pic2.zhimg.com/80/v2-0a21bff27b5f037748292aa338965d65_720w.jpg)

#### 2.2映射服务器节点

将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或唯一主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假设我们将四台服务器使用ip地址哈希后在环空间的位置如下：

![img](https://pic4.zhimg.com/80/v2-252cc4ed5bbb07e5e1e3b27c5eda0d23_720w.jpg)

#### 2.3映射数据

现在我们将objectA、objectB、objectC、objectD四个对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上,然后从数据所在位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。

![img](https://pic3.zhimg.com/80/v2-0fb33fe30c7a05eee2abe3784a42f98a_720w.jpg)

#### 2.4服务器的删除与添加

2.4.1如果此时NodeC宕机了，此时Object A、B、D不会受到影响，只有Object C会重新分配到Node D上面去，而其他数据对象不会发生变化

2.4.2如果在环境中新增一台服务器Node X，通过hash算法将Node X映射到环中，通过按顺时针迁移的规则，那么Object C被迁移到了Node X中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。

![img](https://pic4.zhimg.com/80/v2-bf7daae4aa145478dd55fc339ee57ec7_720w.jpg)

#### 2.5.虚拟节点

到目前为止一致性hash也可以算做完成了，但是有一个问题还需要解决，那就是**平衡性**。从下图我们可以看出，当服务器节点比较少的时候，会出现一个问题，就是此时必然造成大量数据集中到一个节点上面，极少数数据集中到另外的节点上面。

![img](https://pic4.zhimg.com/80/v2-0ce62cf40bcc5f980cafe285dafe0633_720w.jpg)

为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以先确定每个物理节点关联的虚拟节点数量，然后在ip或者主机名后面增加编号。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：

![img](https://pic1.zhimg.com/80/v2-5d9cdea01cb4b44162aa41980345e8ac_720w.jpg)

同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。每个物理节点关联的虚拟节点数量就根据具体的生产环境情况在确定。

### 8 Reactor单线程模型

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019101518351710.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191015183553223.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5ncWlhbmZlbmc=,size_16,color_FFFFFF,t_70)

一个Acceptor线程，监听Accept事件，负责接收客户端的连接SocketChannel，SocketChannel注册到Selector上并关心可读可写事件。
一个Reactor线程，负责轮训selector，将selector注册的就绪事件的key读取出来，拿出attach任务Handler根据事件类型分别去执行读写等。

单线程模型的瓶颈：
比如：拿一个客户端来说，进行多次请求，如果Handler中数据读出来后处理的速度比较慢（非IO操作：解码-计算-编码-返回）会造成客户端的请求被积压，导致响应变慢！
所以引入Reactor多线程模型！

### 9 Reactor多线程模型

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019101518360335.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191015183606481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5ncWlhbmZlbmc=,size_16,color_FFFFFF,t_70)

Reactor多线程就是把Handler中的IO操作，非IO操作分开。
操作IO的线程称为IO线程，操作非IO的线程叫做工作线程。
客户端的请求（IO操作：读取出来的数据）可以直接放进工作线程池（非IO操作：解码-计算-编码-返回）中，这样异步处理，客户端发送的请求就得到返回了不会一直阻塞在Handler中。
但是当用户进一步增加的时候，Reactor线程又会出现瓶颈，因为Reactor中既有IO操作，又要响应连接请求。为了分担Reactor的负担，所以引入了主从Reactor模型!

### 10 主从Reactor模型

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191015183613613.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191015183616830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5ncWlhbmZlbmc=,size_16,color_FFFFFF,t_70)

 主Reactor用于响应连接请求，从Reactor用于处理IO操作请求！
  特点是：服务端用于接收客户端连接的不再是1个单独的NIO线程（Acceptor线程），而是一个独立的NIO线程池。
  Acceptor线程池接收到客户端TCP连接请求处理完成后（可能包含接入认证等），将新创建的SocketChannel注册到I/O线程池（sub reactor线程池）的某个I/O线程上，由它负责SocketChannel的读写和编解码工作。
  Acceptor线程池只用于客户端的登录、握手和安全认证，一旦链路建立成功，就将链路注册到后端subReactor线程池的I/O线程上，有I/O线程负责后续的I/O操作。
  第三种模型比起第二种模型，是将Reactor分成两部分，mainReactor负责监听server socket，accept新连接，并将建立的socket分派给subReactor。subReactor负责多路分离已连接的socket，读写网 络数据，对业务处理功能，其扔给worker线程池完成。通常，subReactor个数上可与CPU个数等同。

### 11 分布式事务模型之XA和TCC的区别和联系？

XA-DTP模型
  最早的分布式事务模型是 X/Open 国际联盟提出的 X/Open Distributed Transaction Processing（DTP）模型，也就是大家常说的 X/Open XA 协议，简称XA 协议。
  DTP 模型中包含一个全局事务管理器（TM，Transaction Manager）和多个资源管理器（RM，Resource Manager）。全局事务管理器负责管理全局事务状态与参与的资源，协同资源一起提交或回滚；资源管理器则负责具体的资源操作。

TCC模型
TCC（Try-Confirm-Cancel）分布式事务模型相对于 XA 等传统模型，其特征在于它不依赖资源管理器（RM）对分布式事务的支持，而是通过对业务逻辑的分解来实现分布式事务。
Try-Confirm-Cancel
Try 操作对应2PC 的一阶段准备（Prepare）；Confirm 对应 2PC 的二阶段提交（Commit），Cancel 对应 2PC 的二阶段回滚（Rollback），可以说 TCC 就是应用层的 2PC。

### 12 高并发下接口幂等性解决方案

一、背景 
      我们实际系统中有很多操作，是不管做多少次，都应该产生一样的效果或返回一样的结果。 例如1. 前端重复提交选中的数据，应该后台只产生对应这个数据的一个反应结果；2. 我们发起一笔付款请求，应该只扣用户账户一次钱，当遇到网络重发或系统bug重发，也应该只扣一次钱；3. 发送消息，也应该只发一次，同样的短信发给用户，用户会哭的；4. 创建业务订单，一次业务请求只能创建一个，创建多个就会出大问题等等很多重要的情况都需要幂等的特性来支持。 
二、幂等性概念 
      幂等（idempotent、idempotence）是一个数学与计算机学概念，常见于抽象代数中。 在编程中.一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。这些函数不会影响系统状态，也不用担心重复执行会对系统造成改变。例如，“getUsername()和setTrue()”函数就是一个幂等函数. 更复杂的操作幂等保证是利用唯一交易号(流水号)实现. 我的理解：幂等就是一个操作，不论执行多少次，产生的效果和返回的结果都是一样的 
三、技术方案 
查询操作：查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select是天然的幂等操作；
删除操作：删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个) ；
唯一索引，防止新增脏数据。比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建资金账户多个，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录。要点：唯一索引或唯一组合索引来防止新增数据存在脏数据（当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可）；
token机制，防止页面重复提交。业务要求： 页面的数据只能被点击提交一次；发生原因： 由于重复点击或者网络重发，或者nginx重发等情况会导致数据被重复提交；解决办法： 集群环境采用token加redis(redis单线程的，处理需要排队)；单JVM环境：采用token加redis或token加jvm内存。处理流程：1. 数据提交前要向服务的申请token，token放到redis或jvm内存，token有效时间；2. 提交后后台校验token，同时删除token，生成新的token返回。token特点：要申请，一次有效性，可以限流。注意：redis要用删除操作来判断token，删除成功代表token校验通过，如果用select+delete来校验token，存在并发问题，不建议使用；
悲观锁——获取数据的时候加锁获取。select * from table_xxx where id='xxx' for update; 注意：id字段一定是主键或者唯一索引，不然是锁表，会死人的悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用； 
乐观锁——乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。乐观锁的实现方式多种多样可以通过version或者其他状态条件：1. 通过版本号实现update table_xxx set name=#name#,version=version+1 where version=#version#如下图(来自网上)；2. 通过条件限制 update table_xxx set avai_amount=avai_amount-#subAmount# where avai_amount-#subAmount# >= 0要求：quality-#subQuality# >= ，这个情景适合不用版本号，只更新是做数据安全校验，适合库存模型，扣份额和回滚份额，性能更高；

![img](https://img-blog.csdn.net/20180715225458309?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE2MzU0OTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)




      注意：乐观锁的更新操作，最好用主键或者唯一索引来更新,这样是行锁，否则更新时会锁表，上面两个sql改成下面的两个更好 

```
update table_xxx set name=#name#,version=version+1 where id=#id# and version=#version#；
update table_xxx set avai_amount=avai_amount-#subAmount# where id=#id# and avai_amount-#subAmount# >= 0；
```

​    7.分布式锁——还是拿插入数据的例子，如果是分布是系统，构建全局唯一索引比较困难，例如唯一性的字段没法确定，这时候可以引入分布式锁，通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁，这样其实是把多线程并发的锁的思路，引入多多个系统，也就是分布式系统中得解决思路。要点：某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户ID+后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁(分布式锁要第三方系统提供)；



source+seq在数据库里面做唯一索引，防止多次付款(并发时，只能处理一个请求) 。重点：对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源source，一个是来源方序列号seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果；没有处理过，进行相应处理，返回结果。注意，为了幂等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理了。 
四、总结
      幂等与你是不是分布式高并发还有JavaEE都没有关系。关键是你的操作是不是幂等的。一个幂等的操作典型如：把编号为5的记录的A字段设置为0这种操作不管执行多少次都是幂等的。一个非幂等的操作典型如：把编号为5的记录的A字段增加1这种操作显然就不是幂等的。要做到幂等性，从接口设计上来说不设计任何非幂等的操作即可。譬如说需求是：当用户点击赞同时，将答案的赞同数量+1。改为：当用户点击赞同时，确保答案赞同表中存在一条记录，用户、答案。赞同数量由答案赞同表统计出来。总之幂等性应该是合格程序员的一个基因，在设计系统时，是首要考虑的问题，尤其是在像支付宝，银行，互联网金融公司等涉及的都是钱的系统，既要高效，数据也要准确，所以不能出现多扣款，多打款等问题，这样会很难处理，用户体验也不好。 

### 13 GIT分支管理

#### GIT分支简介：

　　Git 把我们之前每次提交的版本串成一条时间线，这条时间线就是一个分支。截止到目前只有一条时间线，在git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。
(1) 一开始的时候，master分支是一条线，git用master指向最新的提交，再用HEAD指向master，就能确定当前分支，以及当前分支的提交点：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222143941869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)



每次提交，master分支都会向前移动一步，这样，随着你不断提交，master分支的线也越来越长。
(2) 当我们创建新的分支，例如dev时，git新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev上：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222144045155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

git创建一个分支很快，因为除了增加一个dev指针，改变HEAD的指向，工作区的文件都没有任何变化。
(3) 不过，从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222144114527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

(4) 假如我们在dev上的工作完成了，就可以把dev合并到master上。git怎么合并呢？最简单的方法，就是直接把master指向dev的当前提交，就完成了合并：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222144142299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

git合并分支也很快，就改改指针，工作区内容也不变。
(5) 合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222144224941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

#### GIT分支实例：

(1) 执行如下命令可以查看当前有几个分支并且看到在哪个分支下工作。

```
ubantu@ubantu-virtual-machine:~/git_folder$ git branch * master ubantu@ubantu-virtual-machine:~/git_folder$ 
```

```
ubantu@ubantu-virtual-machine:~/git_folder$ git checkout -b dev
Switched to a new branch 'dev' 
```

(2) 下面创建一个分支dev并切换到其上进行工作。

git branch：查看分支（当前所在分支前面会有一个星号）
git checkout 分支名：切换分支
git branch -b 分支名：创建分支
git branch -d 分支名：删除分支

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222144929531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

git branch：查看分支（当前所在分支前面会有一个星号）
git checkout 分支名：切换分支
git branch -b 分支名：创建分支
git branch -d 分支名：删除分支

(3) 下面我们修改first.txt内容，在里面添加一行，并进行提交。

```
ubantu@ubantu-virtual-machine:~/git_folder$ cat first.txt 
this is my first data
this is my second data
this is my third data
this is my forth data
add one data
ubantu@ubantu-virtual-machine:~/git_folder$ git add first.txt
ubantu@ubantu-virtual-machine:~/git_folder$ git commit -m "dev branch submit"
[dev 3f6bc28] dev branch submit
 1 file changed, 1 insertion(+)
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019022214540644.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

(4) dev分支的工作完成，我们就可以切换回master分支：

 * ```
   ubantu@ubantu-virtual-machine:~/git_folder$ git checkout master 
     Switched to branch 'master'
     ubantu@ubantu-virtual-machine:~/git_folder$ git branch 
   dev
   * master
   ```

  ```
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222145628247.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

查看first.txt，发现添加的内容没有了。因为那个提交是在dev分支上，而master分支此刻的提交点并没有变：

  ```

ubantu@ubantu-virtual-machine:~/git_folder$ cat first.txt
this is my first data
this is my second data
this is my third data
this is my forth data
ubantu@ubantu-virtual-machine:~/git_folder$ 

```
(5) 现在，我们把dev分支的工作成果合并到master分支上：

```

ubantu@ubantu-virtual-machine:~/git_folder$ git merge dev
Updating d7df419..3f6bc28
Fast-forward
 first.txt | 1 +
 1 file changed, 1 insertion(+)
ubantu@ubantu-virtual-machine:~/git_folder$ 

```
git merge命令用于合并指定分支到当前分支。合并后，再查看first.txt的内容，就可以看到，和dev分支的最新提交是完全一样的。

```

ubantu@ubantu-virtual-machine:~/git_folder$ cat first.txt
this is my first data
this is my second data
this is my third data
this is my forth data
add one data
ubantu@ubantu-virtual-machine:~/git_folder$ 

```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222145916747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)


注意到上面的Fast-forward信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。
(6) 合并完成后，就可以放心地删除dev分支了，删除后，查看branch，就只剩下master分支了

* ```
  ubantu@ubantu-virtual-machine:~/git_folder$ git branch -d dev
  Deleted branch dev (was 3f6bc28).
  ubantu@ubantu-virtual-machine:~/git_folder$ git branch
  
  * master
    ubantu@ubantu-virtual-machine:~/git_folder$ 
```


  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222150236206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

#### 解决冲突

合并冲突说明：

某个分支修改了一个文件，在未合并到主分支之前，这个文件也被其他分支修改，此时系统不知道以哪个分支修改后的内容为准来合并到主分支，因此在合并分支时就会产生冲突。


(1) 再创建一个新分支dev。

```
ubantu@ubantu-virtual-machine:~/git_folder$ git checkout -b dev
Switched to a new branch 'dev'
ubantu@ubantu-virtual-machine:~/git_folder$ 
```


(2) 修改first.txt内容，并进行提交。

```
ubantu@ubantu-virtual-machine:~/git_folder$ cat first.txt
this is my first data
this is my second data
this is my third data
this is my forth data
add one data
add another data
ubantu@ubantu-virtual-machine:~/git_folder$ git add first.txt
ubantu@ubantu-virtual-machine:~/git_folder$ git commit -m "dev branch second submit"
[dev d9e0e8c] dev branch second submit
 1 file changed, 1 insertion(+)
ubantu@ubantu-virtual-machine:~/git_folder$ 
```


(3) 切换回master分支。

```
ubantu@ubantu-virtual-machine:~/git_folder$ git checkout master
Switched to branch 'master'
ubantu@ubantu-virtual-machine:~/git_folder$
```


(4) 在master主分支的first.txt添加一行内容并进行提交。

```
ubantu@ubantu-virtual-machine:~/git_folder$ cat first.txtthis is my first data
this is my second data
this is my third data
this is my forth data
add one data
add another data in master
ubantu@ubantu-virtual-machine:~/git_folder$ git add first.txt
ubantu@ubantu-virtual-machine:~/git_folder$ git commit -m "master branch submit"
[master c19bc13] master branch submit
 1 file changed, 1 insertion(+)
ubantu@ubantu-virtual-machine:~/git_folder$
```


现在，master分支和dev分支各自都分别有新的提交，变成了这样：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222162948970.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

这种情况下，git无法执行“快速合并”，只能试图把各自的修改合并起来，但这种合并就可能会有冲突。

(5) 执行如下命令尝试将dev分支合并到master分支上来。

```
ubantu@ubantu-virtual-machine:~/git_folder$ git merge dev
Auto-merging first.txt
CONFLICT (content): Merge conflict in first.txt
Automatic merge failed; fix conflicts and then commit the result.
ubantu@ubantu-virtual-machine:~/git_folder$
```


提示我们自动合并产生冲突，冲突文件为first.txt，需要手动修正冲突后再提交

(6) git status也可以告诉我们冲突的文件：

```
ubantu@ubantu-virtual-machine:~/git_folder$ git status
On branch master
You have unmerged paths.
  (fix conflicts and run "git commit")

Unmerged paths:
  (use "git add <file>..." to mark resolution)

	both modified:   first.txt

no changes added to commit (use "git add" and/or "git commit -a")
ubantu@ubantu-virtual-machine:~/git_folder$
```

提示我们有尚未合并的路径，需要解决冲突后运行git commit命令
有两个人修改了first.txt文件

(7) 查看code.txt的内容。

![image-20200306235051233](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200306235051233.png)

(8) git用以下标记出不同分支的内容(我们手动删除以下内容即可)

```
<<<<<<<， =======， >>>>>>>
```

(9) 我们修改如下后保存

```
ubantu@ubantu-virtual-machine:~/git_folder$ cat first.txt
this is my first data
this is my second data
this is my third data
this is my forth data
add one data
add another data in master
add another data
ubantu@ubantu-virtual-machine:~/git_folder$ 
```

(10) 再提交。

```
ubantu@ubantu-virtual-machine:~/git_folder$ git commit -m "solve merge conflict" 
[master 194ad75] solve merge conflict
ubantu@ubantu-virtual-machine:~/git_folder$ 
```


(10) 现在，master分支和dev分支变成了下图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222165746269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNTE3MjIw,size_16,color_FFFFFF,t_70)

(11) 用带参数的git log也可以看到分支的合并情况：

* ```
  ubantu@ubantu-virtual-machine:~/git_folder$ git log --graph --pretty=oneline
  
  *   194ad7508eb5656a243bf93ca36f6c23412566e1 solve merge conflict
      |\  
      | * d9e0e8cc0026d256043938ef38ecfe9a1e11bbc0 dev branch second submit
  *   | c19bc130daf086d984aa1a7395a95a2a580d1998 master branch submit
      |/  
  *   3f6bc28880f2edd636e5b0527e18f8225bef4209 dev branch submit
  *   d7df4195d600a5c9d414519c757e91dae50df39c 删除文件second.txt
  *   f696849773147218c4d72dfb62932be47a079bbc 版本4
  *   724a2201044ae9f6fc5f2905fd3b032001a71d51 版本3
  *   ca3a776239c5532e728ffae0be467584a12215b0 版本2
  *   22355f7dffd474ed51ac033e825c67b4393eaa87 版本1
      ubantu@ubantu-virtual-machine:~/git_folder$ 
  ```


  (12) 最后工作完成，可以删除dev分支。

* ```
  ubantu@ubantu-virtual-machine:~/git_folder$ git branch -d dev
  Deleted branch dev (was d9e0e8c).
  ubantu@ubantu-virtual-machine:~/git_folder$ git branch
  
  * master
    ubantu@ubantu-virtual-machine:~/git_folder$
  ```

#### 分支管理策略

通常，合并分支时，如果可能，git会用fast forward模式，但是有些时候快速合并不能成功而且没有冲突，这个时候git会帮你合并，并做一次新的提交。但这种模式下，删除分支后，会丢掉分支信息。（在分支上新建并修改某个文件内容之后，还未合并到主分支之前，此时主分支修改了其他文件，然后合并就会出现此种情况）


Git 分支管理常见三种方式
TBD（Trunk-based development、单主干）
GitHub flow
git-flow

1. TBD
   所有团队成员都在单个主干分支上进行开发。
   发布时，先考虑使用标签 Tag， 如果打标签不能满足要求，则从主干创建发布分支。
   Bug在主干上修复，然后挑选时机发布到 发布分支上
   优点: 分支少，开发人员不需要频繁在不同的分支之间切换。
   缺点: 因为主干分支是所有开发人员公用的，一个开发人员引入的 bug 可能对其他很多人造成影响。
2. GitHub flow
   GitHub使用的一种简单流程，使用两类分支，Master、代码修改分支，主要对应GitHub的pull、request功能。

master分支
包含稳定的代码，该分支已经或即将被部署到生产环境，分支的作用是提供一个稳定可靠的代码基础。任何开发人员都不允许把未测试或未审查的代码直接提交到 master 分支。

代码修改分支
当需要进行任何修改时（包括 bug 修复、hotfix、新功能开发等），总是从 master 分支创建新分支。

分支合并流程
当新分支中的代码全部完成之后，通过 GitHub 提交一个新的 pull request。团队中的其他人员会对代码进行审查，提出相关的修改意见。由持续集成服务器对新分支进行自动化测试。当代码通过自动化测试和代码审查之后，该分支的代码被合并到 master 分支。再从 master 分支部署到生产环境

3. Git-flow
   目前流传最广的 Git 分支管理实践，围绕的核心概念是版本发布（release）。git-flow 流程中包含 5 类分支，分别是 master、develop、feature、release 和 hotfix。

Master 分支中包含的是可以部署到生产环境中的代码，这一点和 GitHub flow 是相同的

develop 分支中包含的是下个版本需要发布的内容。当 develop 分支集成了足够的新功能和 bug 修复代码之后，通过一个发布流程来完成新版本的发布。发布完成之后，develop 分支的代码会被合并到 master 分支中。

![image-20200306235546434](C:\Users\小硕哥\AppData\Roaming\Typora\typora-user-images\image-20200306235546434.png)

### 14 @Transactional不生效的场景

数据库引擎是否支持事务。

注解所在的类是否被加载。

注解所在的方法是否为public修饰。如果要用在非 public 方法上，可以开启 AspectJ 代理模式。

是否发生了自调用问题。就调该类自己的方法，而没有经过 Spring 的代理类，默认只有在外部调用事务才会生效。Spring 如何在一个事务中开启另一个事务？

所用的数据源是否加载了事务管理器。

@Transactional的扩展配置propagation是否正确。

Propagation.NOT_SUPPORTED： 表示不以事务运行。异常不抛出。

设置触发事务的异常错误。默认回滚的是：RuntimeException，如果想触发其他异常的回滚，需要在注解上配置一下，如：@Transactional(rollbackFor = Exception.class)这个配置仅限于 Throwable 异常类及其子类。



